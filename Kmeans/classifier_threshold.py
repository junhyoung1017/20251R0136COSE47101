import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler, MultiLabelBinarizer
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, hamming_loss, jaccard_score
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.multioutput import MultiOutputClassifier
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import warnings
from sklearn.decomposition import PCA
from xgboost import XGBClassifier
import joblib,os

warnings.filterwarnings('ignore')

# í•œê¸€ í°íŠ¸ ì„¤ì • (matplotlib)
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False

def load_and_explore_data(pkl_path):
    print("ğŸ“‚ ë°ì´í„° ë¡œë”© ì¤‘...")
    df = pd.read_pickle(pkl_path)
    # ì»¬ëŸ¼ ë¶„ë¥˜
    basic_cols = [col for col in df.columns if not col.startswith('bert_')]
    bert_name_cols = [col for col in df.columns if col.startswith('bert_name_')]
    bert_desc_cols = [col for col in df.columns if col.startswith('bert_desc_')]
    
    # íƒ€ê²Ÿ ë³€ìˆ˜ í™•ì¸ (stack_list ì‚¬ìš©)
    if 'stack_list' in df.columns:
        print(f"\nğŸ¯ íƒ€ê²Ÿ ë³€ìˆ˜ (stack_list) ë¶„ì„:")
        
        # ìŠ¤íƒ ë¦¬ìŠ¤íŠ¸ ë¶„ì„
        all_stacks = []
        valid_stack_lists = []
        
        for stack_list in df['stack_list']:
            if isinstance(stack_list, list) and len(stack_list) > 0:
                all_stacks.extend(stack_list)
                valid_stack_lists.append(stack_list)
        
        stack_counts = Counter(all_stacks)
        print(f"ì´ ê³ ìœ  ìŠ¤íƒ ìˆ˜: {len(stack_counts)}") #7
        return df, basic_cols, bert_name_cols, bert_desc_cols, stack_counts, valid_stack_lists

def prepare_features(df, basic_cols, bert_name_cols, bert_desc_cols):
    """
    ëª¨ë¸ í•™ìŠµì„ ìœ„í•œ í”¼ì²˜ ì¤€ë¹„ + BERT ì„ë² ë”© PCA ì ìš©
    """
    print("\nğŸ”§ í”¼ì²˜ ì¤€ë¹„ ì¤‘...")
    exclude_cols = {'user_ID', 'username', 'repo_names', 'description', 'stack', 'stack_list', 'note', 'repo_count'}
    language_cols = [col for col in basic_cols if col not in exclude_cols and df[col].dtype in ['int64', 'float64']]
    print(f"ğŸ¯ ì–¸ì–´ í†µê³„ í”¼ì²˜: {language_cols}")

    basic_info_cols = ['repo_count'] if 'repo_count' in df.columns else []
    print("ğŸ“‰ PCA ì ìš© ì¤‘ (BERT name/desc)...")
    
    # PCA ì ìš© (ì°¨ì› ìˆ˜ë¥¼ ì ë‹¹íˆ ì¤„ì„)
    pca_name = PCA(n_components=0.9, random_state=42)
    pca_desc = PCA(n_components=0.9, random_state=42)

    bert_name_pca = pca_name.fit_transform(df[bert_name_cols])
    bert_desc_pca = pca_desc.fit_transform(df[bert_desc_cols])

    bert_name_df = pd.DataFrame(bert_name_pca, columns=[f'pca_name_{i}' for i in range(bert_name_pca.shape[1])])
    bert_desc_df = pd.DataFrame(bert_desc_pca, columns=[f'pca_desc_{i}' for i in range(bert_desc_pca.shape[1])])

    df_pca = pd.concat([df.reset_index(drop=True), bert_name_df, bert_desc_df], axis=1)
    feature_columns = language_cols + basic_info_cols+list(bert_name_df.columns) + list(bert_desc_df.columns)
    X = df_pca[feature_columns].copy()

    print(f"ğŸ“Š ì´ í”¼ì²˜ ìˆ˜ (PCA ì ìš© í›„): {len(feature_columns)}")
    print(f"â€¢ ì–¸ì–´ í†µê³„: {len(language_cols)}ê°œ")
    print(f"â€¢ ê¸°ë³¸ ì •ë³´: {len(basic_info_cols)}ê°œ")
    print(f"â€¢ BERT PCA name: {bert_name_pca.shape[1]}ê°œ")
    print(f"â€¢ BERT PCA desc: {bert_desc_pca.shape[1]}ê°œ")

    return X, feature_columns, language_cols, pca_name, pca_desc

def prepare_target_onehot(df, min_samples=10):
    """
    ìŠ¤íƒì„ ì›í•«ì¸ì½”ë”©ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
    """
    print(f"\nğŸ¯ ìŠ¤íƒ ì›í•«ì¸ì½”ë”© ì¤€ë¹„ ì¤‘ (ìµœì†Œ {min_samples}ê°œ ìƒ˜í”Œ)...")
    
    # ìœ íš¨í•œ stack_listë§Œ ì„ íƒ
    valid_mask = df['stack_list'].apply(lambda x: isinstance(x, list) and len(x) > 0)
    print(f"ìœ íš¨í•œ íƒ€ê²Ÿ ë°ì´í„°: {valid_mask.sum()}ê°œ")
    
    # ìŠ¤íƒë³„ ë¹ˆë„ ê³„ì‚°
    all_stacks = []
    for stack_list in df.loc[valid_mask, 'stack_list']:
        all_stacks.extend(stack_list)
    
    stack_counts = Counter(all_stacks)
    frequent_stacks = [stack for stack, count in stack_counts.items() if count >= min_samples]
    print(f"ìµœì†Œ {min_samples}ê°œ ìƒ˜í”Œì„ ê°€ì§„ ìŠ¤íƒ ìˆ˜: {len(frequent_stacks)}")
    
    
    # ë¹ˆë²ˆí•œ ìŠ¤íƒë§Œ í¬í•¨í•˜ëŠ” ìŠ¤íƒ ë¦¬ìŠ¤íŠ¸ í•„í„°ë§
    filtered_stack_lists = []
    filtered_indices = []
    
    for idx, stack_list in df.loc[valid_mask, 'stack_list'].items():
        filtered_list = [stack for stack in stack_list if stack in frequent_stacks]
        if filtered_list:  # ë¹ˆ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹Œ ê²½ìš°ë§Œ í¬í•¨
            filtered_stack_lists.append(filtered_list)
            filtered_indices.append(idx)
    
    print(f"í•„í„°ë§ í›„ ë°ì´í„°: {len(filtered_indices)}ê°œ") #
    
    # MultiLabelBinarizerë¡œ ì›í•«ì¸ì½”ë”©
    mlb = MultiLabelBinarizer()
    y_onehot = mlb.fit_transform(filtered_stack_lists)
    
    # ìœ íš¨í•œ ë§ˆìŠ¤í¬ ìƒì„± ë…¸ì´ì¦ˆ ì œê±°. ì˜ ë“±ì¥í•˜ì§€ ì•ŠëŠ” ìŠ¤íƒ ì œê±°ê±°
    final_valid_mask = pd.Series(False, index=df.index)
    final_valid_mask.iloc[filtered_indices] = True
    print(f"âœ… ì›í•«ì¸ì½”ë”© ì™„ë£Œ")
    print(f"íƒ€ê²Ÿ shape: {y_onehot.shape}")
    print(f"ìŠ¤íƒ í´ë˜ìŠ¤: {list(mlb.classes_)}")
    # í´ë˜ìŠ¤ë³„ ìƒ˜í”Œ ìˆ˜ í™•ì¸
    class_counts = y_onehot.sum(axis=0)
    print(f"\ní´ë˜ìŠ¤ë³„ ìƒ˜í”Œ ìˆ˜:")
    for i, (class_name, count) in enumerate(zip(mlb.classes_, class_counts)):
        print(f"  {class_name}: {count}ê°œ")
    return y_onehot, final_valid_mask, mlb, frequent_stacks

def train_multilabel_models(X_train, y_train, X_test, y_test, mlb):
    """
    ë©€í‹°ë¼ë²¨ ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
    """
    print("\nğŸ¤– ë©€í‹°ë¼ë²¨ ëª¨ë¸ í•™ìŠµ ì¤‘...")
    
    base_models = {
        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1, class_weight='balanced'),
        'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced'),
        'Gradient Boosting': GradientBoostingClassifier(n_estimators=50, random_state=42),
        'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42),
    }
    
    results = {}
    
    for name, base_model in base_models.items():
        print(f"\nğŸ”„ {name} (ë©€í‹°ë¼ë²¨) í•™ìŠµ ì¤‘...")
        
        '''# Gradient Boosting íŠœë‹
        if name == 'Gradient Boosting':
            param_grid = {
                'estimator__n_estimators': [100, 200],
                'estimator__learning_rate': [0.05, 0.1],
                'estimator__max_depth': [3, 5],
                'estimator__subsample': [0.8, 1.0],
                
            }
            model = GridSearchCV(
                estimator=MultiOutputClassifier(base_model, n_jobs=-1),
                param_grid=param_grid,
                scoring='f1_micro',
                cv=3,
                n_jobs=-1,
                verbose=1
            )

        # XGBoost íŠœë‹
        elif name == 'XGBoost':
            param_grid = {
                'estimator__n_estimators': [100, 200],
                'estimator__learning_rate': [0.05, 0.1],
                'estimator__max_depth': [3, 5],
                'estimator__subsample': [0.8, 1.0],
                'estimator__colsample_bytree': [0.8, 1.0],
                
                }
            model = GridSearchCV(
                estimator=MultiOutputClassifier(base_model, n_jobs=-1),
                param_grid=param_grid,
                scoring='f1_micro',
                cv=3,
                
                n_jobs=-1,
                verbose=1
            )

        # ë‚˜ë¨¸ì§€ ëª¨ë¸ì€ ê¸°ë³¸ MultiOutputClassifier ì‚¬ìš©
        else:'''
        model = MultiOutputClassifier(base_model, n_jobs=-1)
        
        # ëª¨ë¸ í•™ìŠµ
        model.fit(X_train, y_train)
        if hasattr(model, "best_params_"):
            print(f"ğŸ“Œ {name} ìµœì  íŒŒë¼ë¯¸í„°: {model.best_params_}")
        # ì˜ˆì¸¡ (í™•ë¥ )
        y_pred_proba = model.predict_proba(X_test)
        
        # ê° í´ë˜ìŠ¤ë³„ë¡œ ìµœê³  í™•ë¥ ì„ ê°€ì§„ ì˜ˆì¸¡ì„ ì„ íƒ
        y_pred_binary = np.zeros_like(y_test)
        
        # ë°©ë²• 1: ê° ìƒ˜í”Œë§ˆë‹¤ ê°€ì¥ ë†’ì€ í™•ë¥ ì„ ê°€ì§„ í´ë˜ìŠ¤ë¥¼ 1ë¡œ ì„¤ì •
        for i in range(len(y_test)):
            if len(y_pred_proba) > 0:
                sample_probs = []
                for j in range(y_train.shape[1]):  # ê° í´ë˜ìŠ¤ì— ëŒ€í•´
                    if len(y_pred_proba[j][i]) > 1:  # ì´ì§„ ë¶„ë¥˜ì˜ ê²½ìš°
                        sample_probs.append(y_pred_proba[j][i][1])  # ì–‘ì„± í´ë˜ìŠ¤ í™•ë¥ 
                    else:
                        sample_probs.append(y_pred_proba[j][i][0])
                
                # ìƒìœ„ í™•ë¥ ì„ ê°€ì§„ í´ë˜ìŠ¤ë“¤ì„ ì„ íƒ (ì„ê³„ê°’ ê¸°ë°˜)
                threshold = 0.6  # ì¡°ì • ê°€ëŠ¥í•œ ì„ê³„ê°’
                for j, prob in enumerate(sample_probs):
                    if prob > threshold:
                        y_pred_binary[i, j] = 1
                
                # ìµœì†Œí•œ í•˜ë‚˜ì˜ í´ë˜ìŠ¤ëŠ” ì˜ˆì¸¡ë˜ë„ë¡ ë³´ì¥
                if y_pred_binary[i].sum() == 0:
                    best_class = np.argmax(sample_probs)
                    y_pred_binary[i, best_class] = 1
        
        # í‰ê°€ ë©”íŠ¸ë¦­ ê³„ì‚°
        hamming = hamming_loss(y_test, y_pred_binary)
        jaccard = jaccard_score(y_test, y_pred_binary, average='samples', zero_division=0)
        
        # ì •í™•í•œ ë§¤ì¹˜ ë¹„ìœ¨ (ëª¨ë“  ë¼ë²¨ì´ ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ë¹„ìœ¨)
        exact_match = np.mean(np.all(y_test == y_pred_binary, axis=1))
        
        # ì»¤ìŠ¤í…€ í‰ê°€: ì˜ˆì¸¡ê³¼ ì‹¤ì œê°€ ê²¹ì¹˜ëŠ” ê²ƒì´ ìˆìœ¼ë©´ ì„±ê³µ
        #flexible_accuracy = calculate_flexible_accuracy(y_test, y_pred_binary)
        
        results[name] = {
            'model': model,
            'hamming_loss': hamming,
            'jaccard_score': jaccard,
            'exact_match': exact_match,
            #'flexible_accuracy': flexible_accuracy,
            'predictions': y_pred_binary,
            'predictions_proba': y_pred_proba
        }
        
        print(f"âœ… {name} ì™„ë£Œ!")
        print(f"   Hamming Loss: {hamming:.4f}")
        print(f"   Jaccard Score: {jaccard:.4f}")
        print(f"   Exact Match: {exact_match:.4f}")
        #print(f"   Flexible Accuracy: {flexible_accuracy:.4f}")
    
    return results

'''def calculate_flexible_accuracy(y_true, y_pred):
    """
    ì˜ˆì¸¡ëœ ìŠ¤íƒ ì¤‘ í•˜ë‚˜ë¼ë„ ì‹¤ì œ ìŠ¤íƒê³¼ ì¼ì¹˜í•˜ë©´ ì •ë‹µìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” ì •í™•ë„
    """
    correct = 0
    total = len(y_true)
    
    for i in range(total):
        # ì‹¤ì œ ìŠ¤íƒ (1ì¸ ìœ„ì¹˜)
        true_stacks = set(np.where(y_true[i] == 1)[0])
        # ì˜ˆì¸¡ëœ ìŠ¤íƒ (1ì¸ ìœ„ì¹˜)
        pred_stacks = set(np.where(y_pred[i] == 1)[0])
        
        # êµì§‘í•©ì´ ìˆìœ¼ë©´ ì„±ê³µ
        if len(true_stacks.intersection(pred_stacks)) > 0:
            correct += 1
    
    return correct / total'''

def evaluate_model_performance(results, y_test, mlb):
    """
    ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ë° ì¶œë ¥
    """
    print("\nğŸ† ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ:")
    print("-" * 80)
    print(f"{'Model':<20} | {'Hamming':<8} | {'Jaccard':<8} | {'Exact':<8}")
    print("-" * 80)
    
    best_model_name = None
    best_score = 0
    
    for name, result in results.items():
        # flexible_acc = result['flexible_accuracy']
        exact_match = result['exact_match']
        print(f"{name:<20} | {result['hamming_loss']:<8.4f} | {result['jaccard_score']:<8.4f} | {result['exact_match']:<8.4f}")
        
        # Flexible Accuracyë¥¼ ê¸°ì¤€ìœ¼ë¡œ ìµœê³  ëª¨ë¸ ì„ íƒ
        if exact_match > best_score:
            best_score = exact_match
            best_model_name = name
    
    print("-" * 80)
    print(f"ğŸ¥‡ ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model_name} (Exact_match: {best_score:.4f})")
    
    # ìƒì„¸ ë¶„ì„
    best_result = results[best_model_name]
    y_pred = best_result['predictions']
    
    print(f"\nğŸ“Š {best_model_name} ìƒì„¸ ë¶„ì„:")
    
    # í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ë¶„ì„
    print("\ní´ë˜ìŠ¤ë³„ ì„±ëŠ¥:")
    for i, class_name in enumerate(mlb.classes_):
        true_positive = np.sum((y_test[:, i] == 1) & (y_pred[:, i] == 1))
        false_positive = np.sum((y_test[:, i] == 0) & (y_pred[:, i] == 1))
        false_negative = np.sum((y_test[:, i] == 1) & (y_pred[:, i] == 0))
        
        precision = true_positive / (true_positive + false_positive) if (true_positive + false_positive) > 0 else 0
        recall = true_positive / (true_positive + false_negative) if (true_positive + false_negative) > 0 else 0
        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
        
        print(f"  {class_name:<15}: Precision={precision:.3f}, Recall={recall:.3f}, F1={f1:.3f}")
    
    return best_result['model'], best_model_name, best_result

def visualize_results(results, mlb):
    """
    ê²°ê³¼ ì‹œê°í™”
    """
    print("\nğŸ“ˆ ê²°ê³¼ ì‹œê°í™” ì¤‘...")
    
    # ì„±ëŠ¥ ë¹„êµ ì°¨íŠ¸
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    model_names = list(results.keys())
    metrics = {
        'Hamming Loss': [results[name]['hamming_loss'] for name in model_names],
        'Jaccard Score': [results[name]['jaccard_score'] for name in model_names],
        'Exact Match': [results[name]['exact_match'] for name in model_names],
        # 'Flexible Accuracy': [results[name]['flexible_accuracy'] for name in model_names]
    }
    
    colors = ['skyblue', 'lightcoral', 'lightgreen', 'gold']
    
    for idx, (metric_name, values) in enumerate(metrics.items()):
        row = idx // 2
        col = idx % 2
        
        bars = axes[row, col].bar(model_names, values, color=colors[idx], alpha=0.7)
        axes[row, col].set_title(f'{metric_name}')
        axes[row, col].set_ylabel('Score')
        axes[row, col].tick_params(axis='x', rotation=45)
        
        # ê°’ í‘œì‹œ
        for bar, value in zip(bars, values):
            axes[row, col].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                               f'{value:.3f}', ha='center', va='bottom')
    
    plt.tight_layout()
    plt.show()

def main():
    # íŒŒì¼ ê²½ë¡œ ì„¤ì •
    pkl_path = 'C:/Users/jun01/OneDrive/ë°”íƒ• í™”ë©´/ê³ ë ¤ëŒ€/ë°ê³¼/TermProject/20251R0136COSE47101/Kmeans/pkl_data/github_profiles_with_bert_processed_v2.pkl'
    
    try:
        # 1. ë°ì´í„° ë¡œë“œ ë° íƒìƒ‰
        df, basic_cols, bert_name_cols, bert_desc_cols, stack_counts, valid_stack_lists = load_and_explore_data(pkl_path)
        
        if stack_counts is None:
            return
        
        # 2. í”¼ì²˜ ì¤€ë¹„
        X, feature_columns, language_cols, pca_name, pca_desc = prepare_features(df, basic_cols, bert_name_cols, bert_desc_cols)
        
        # 3. íƒ€ê²Ÿ ì¤€ë¹„ - ì›í•«ì¸ì½”ë”© ë°©ì‹
        print("\nğŸ¯ ì›í•«ì¸ì½”ë”© ë°©ì‹ ì„ íƒ")
        y, valid_mask, mlb, frequent_stacks = prepare_target_onehot(df, min_samples=8)
        
        # ìœ íš¨í•œ ë°ì´í„°ë§Œ ì„ íƒ
        X = X.loc[valid_mask].reset_index(drop=True)
        
        print(f"\nğŸ“Š ìµœì¢… ë°ì´í„° í¬ê¸°: {X.shape}")
        print(f"íƒ€ê²Ÿ ë°ì´í„° í¬ê¸°: {y.shape}")
        
        # 4. ë°ì´í„° ë¶„í• 
        print("\nğŸ”€ ë°ì´í„° ë¶„í•  ì¤‘...")
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )
        
        print(f"í•™ìŠµ ë°ì´í„°: {X_train.shape}, {y_train.shape}")
        print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {X_test.shape}, {y_test.shape}")
        
        # 5. í”¼ì²˜ ìŠ¤ì¼€ì¼ë§
        print("\nâš–ï¸ í”¼ì²˜ ìŠ¤ì¼€ì¼ë§ ì¤‘...")
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        # 6. ë©€í‹°ë¼ë²¨ ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
        results = train_multilabel_models(X_train_scaled, y_train, X_test_scaled, y_test, mlb)
        
        # 7. ëª¨ë¸ ì„±ëŠ¥ í‰ê°€
        best_model, best_model_name, best_result = evaluate_model_performance(results, y_test, mlb)
        
        # 8. ê²°ê³¼ ì‹œê°í™”
        visualize_results(results, mlb)
        
        # print(f"Flexible Accuracy: {best_result['flexible_accuracy']:.4f}")
        print(f"Exact Match: {best_result['exact_match']:.4f}")
        
        print("\nğŸ‰ ìŠ¤íƒ ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")
        
        # 10. ì˜ˆì‹œ ì˜ˆì¸¡ ì¶œë ¥
        print("\nğŸ” ì˜ˆì¸¡ ì˜ˆì‹œ (ì²˜ìŒ 5ê°œ ìƒ˜í”Œ):")
        y_pred = best_result['predictions']
        for i in range(min(5, len(y_test))):
            true_stacks = [mlb.classes_[j] for j in range(len(mlb.classes_)) if y_test[i, j] == 1]
            pred_stacks = [mlb.classes_[j] for j in range(len(mlb.classes_)) if y_pred[i, j] == 1]
            
            print(f"ìƒ˜í”Œ {i+1}:")
            print(f"  ì‹¤ì œ: {true_stacks}")
            print(f"  ì˜ˆì¸¡: {pred_stacks}")
            print(f"  ì¼ì¹˜: {'âœ…' if len(set(true_stacks).intersection(set(pred_stacks))) > 0 else 'âŒ'}")
            print()
         # ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
        model_save_path = 'C:/Users/jun01/OneDrive/ë°”íƒ• í™”ë©´/ê³ ë ¤ëŒ€/ë°ê³¼/TermProject/20251R0136COSE47101/Kmeans/pkl_data'
        os.makedirs(model_save_path, exist_ok=True)

        # ëª¨ë¸ ì €ì¥ (ì˜ˆ: best_model.pkl)
        model_filename = f"{model_save_path}/best_model2_{best_model_name.replace(' ', '_')}.pkl"
        joblib.dump(best_model, model_filename)
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()