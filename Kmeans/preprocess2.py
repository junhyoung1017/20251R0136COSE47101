import numpy as np
import pandas as pd
from sentence_transformers import SentenceTransformer
import pickle
import re
from typing import Tuple, List
import os
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import MultiLabelBinarizer
# íŒŒì¼ ê²½ë¡œ
file_path = 'C:/Users/jun01/OneDrive/ë°”íƒ• í™”ë©´/ê³ ë ¤ëŒ€/ë°ê³¼/TermProject/20251R0136COSE47101/Kmeans/github_profiles_total_v5.csv'

def split_repos(text: str) -> Tuple[str, str]:
    """
    Repository í…ìŠ¤íŠ¸ë¥¼ ì´ë¦„ê³¼ ì„¤ëª…ìœ¼ë¡œ ë¶„ë¦¬í•˜ëŠ” í•¨ìˆ˜
    """
    if pd.isna(text) or text == '':
        return '', ''
    
    repos = str(text).split('/')  # ê° repo êµ¬ë¶„
    repo_names = []
    descriptions = []
    
    for repo in repos:
        parts = repo.split('::')
        name = parts[0].strip()
        desc = parts[1].strip() if len(parts) > 1 else ''
        
        # ë¹ˆ ë¬¸ìì—´ì´ ì•„ë‹Œ ê²½ìš°ë§Œ ì¶”ê°€
        if name and name != 'nan':
            repo_names.append(name)
        if desc and desc != 'nan':
            descriptions.append(desc)
    
    return ', '.join(repo_names), ', '.join(descriptions)
def process_stack(stack_text: str) -> List[str]:
    """
    Stack í…ìŠ¤íŠ¸ë¥¼ &ìœ¼ë¡œ ë¶„ë¦¬í•˜ê³  ì •ì œí•˜ëŠ” í•¨ìˆ˜
    """
    if pd.isna(stack_text) or stack_text == '':
        return []
    
    # &ìœ¼ë¡œ ë¶„ë¦¬í•˜ê³  ê° ìŠ¤íƒ ì •ì œ
    stacks = [s.strip() for s in str(stack_text).split('&') if s.strip()]
    
    # ë¹ˆ ë¬¸ìì—´ì´ë‚˜ 'nan' ì œê±°
    stacks = [s for s in stacks if s and s.lower() != 'nan']
    
    return stacks
def clean_text(text: str) -> str:
    """
    í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í•¨ìˆ˜ (íŠ¹ìˆ˜ë¬¸ì ì œê±°, ì†Œë¬¸ì ë³€í™˜ ë“±)
    """
    if pd.isna(text) or text == '':
        return ''
    # íŠ¹ìˆ˜ë¬¸ì ì œê±° (ì˜ë¬¸, ìˆ«ì, ê³µë°±ë§Œ ë‚¨ê¹€)
    text = re.sub(r'[^a-zA-Z0-9\s]', ' ', str(text))
    # ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ
    text = re.sub(r'\s+', ' ', text)
    # ì•ë’¤ ê³µë°± ì œê±°
    text = text.strip()
    return text

def main():
    print("ğŸ“ ë°ì´í„° ë¡œë”© ì¤‘...")
    
    # ë°ì´í„° ë¡œë”© (ì¸ì½”ë”© ì—ëŸ¬ ì²˜ë¦¬)
    try:
        with open(file_path, 'r', encoding='utf-8', errors='replace') as f:
            df = pd.read_csv(f)
    except FileNotFoundError:
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
        return
    
    print(f"âœ… ë°ì´í„° ë¡œë”© ì™„ë£Œ. ì´ {len(df)}ê°œ í–‰")
    print(f"ğŸ“Š ì»¬ëŸ¼: {list(df.columns)}")
    
    # 1. ì–¸ì–´ ë°ì´í„° í†µí•©
    print("\nğŸ”„ ì–¸ì–´ ë°ì´í„° í†µí•© ì¤‘...")
    # JavaScript + TypeScript = JS
    if 'JavaScript' in df.columns and 'TypeScript' in df.columns:
        df["JS"] = df[['JavaScript', 'TypeScript']].sum(axis=1)
        df.drop(columns=['JavaScript', 'TypeScript'], inplace=True)
        print("âœ… JavaScript + TypeScript â†’ JS í†µí•© ì™„ë£Œ")
    # C + C++ = C/C++
    if 'C' in df.columns and 'C++' in df.columns:
        df["C/C++"] = df[['C', 'C++']].sum(axis=1)
        df.drop(columns=['C', 'C++'], inplace=True)
        print("âœ… C + C++ â†’ C/C++ í†µí•© ì™„ë£Œ")
    # 2-1. Repository ì´ë¦„ê³¼ ì„¤ëª… ë¶„ë¦¬
    print("\nğŸ“ Repository í…ìŠ¤íŠ¸ ë¶„ë¦¬ ì¤‘...")
    if 'text' in df.columns:
        df[['repo_names', 'description']] = df['text'].apply(lambda x: pd.Series(split_repos(x)))
        df.drop(columns=['text'], inplace=True)
        print("âœ… Repository ì´ë¦„ê³¼ ì„¤ëª… ë¶„ë¦¬ ì™„ë£Œ")
        
        # ë¶„ë¦¬ ê²°ê³¼ í™•ì¸
        print(f"ğŸ“‹ Repository ì´ë¦„ ìƒ˜í”Œ:\n{df['repo_names'].head()}")
        print(f"ğŸ“‹ Description ìƒ˜í”Œ:\n{df['description'].head()}")
   # 2-2. Stack ë¶„ë¦¬ ë° ì •ì œ (ê°œì„ ëœ ë²„ì „)
    if 'stack' in df.columns:
        print("\nğŸ”„ Stack ë¶„ë¦¬ ë° ì •ì œ ì¤‘...")
        
        # ì›ë³¸ stack ë°ì´í„° í™•ì¸
        print(f"ğŸ“‹ ì›ë³¸ Stack ìƒ˜í”Œ:\n{df['stack'].head()}")
        
        # Stackì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë¶„ë¦¬
        df['stack_list'] = df['stack'].apply(process_stack)
        
        # ë¶„ë¦¬ ê²°ê³¼ í™•ì¸
        print(f"ğŸ“‹ ë¶„ë¦¬ëœ Stack ìƒ˜í”Œ:")
        for i in range(min(5, len(df))):
            print(f"   {i+1}. ì›ë³¸: '{df['stack'].iloc[i]}' â†’ ë¶„ë¦¬: {df['stack_list'].iloc[i]}")
        
        # ìŠ¤íƒ í†µê³„ ì •ë³´
        all_stacks = []
        for stack_list in df['stack_list']:
            all_stacks.extend(stack_list)
        
        unique_stacks = list(set(all_stacks))
        print(f"ğŸ“Š ì „ì²´ ê³ ìœ  ìŠ¤íƒ ìˆ˜: {len(unique_stacks)}")
        print(f"ğŸ“Š ê°€ì¥ ë§ì´ ì‚¬ìš©ëœ ìŠ¤íƒ ìƒìœ„ 10ê°œ:")
        
        from collections import Counter
        stack_counts = Counter(all_stacks)
        for stack, count in stack_counts.most_common(10):
            print(f"   {stack}: {count}íšŒ")
        
        print("âœ… Stack ë¶„ë¦¬ ë° ì •ì œ ì™„ë£Œ")
    # 3. ì–¸ì–´ ì»¬ëŸ¼ í™•ì¸
    print("\nğŸ“Š ì–¸ì–´ ë°ì´í„° í™•ì¸ ì¤‘...")
    # ì–¸ì–´ ì»¬ëŸ¼ ì‹ë³„ (ìˆ«ìí˜• ë°ì´í„°ì¸ ì»¬ëŸ¼ë“¤)
    exclude_columns = {'user_ID', 'username', 'repo_count', 'repo_names', 'description', 'stack','stack_list', 'note'}
    language_columns = [col for col in df.columns if col not in exclude_columns and df[col].dtype in ['int64', 'float64']]
    
    print(f"ğŸ¯ ì–¸ì–´ ì»¬ëŸ¼: {language_columns}")
    print("âœ… ì–¸ì–´ ë°ì´í„° í™•ì¸ ì™„ë£Œ")
    
    # 4. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
    print("\nğŸ§¹ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ì¤‘...")
    
    # ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ë° í…ìŠ¤íŠ¸ ì •ì œ
    df['description'] = df['description'].fillna('').apply(clean_text)
    df['repo_names'] = df['repo_names'].fillna('').apply(clean_text)
    
    # ë¹ˆ ë¬¸ìì—´ì„ "no description" ë˜ëŠ” "no repository"ë¡œ ëŒ€ì²´ (BERT ì„ë² ë”©ì„ ìœ„í•´)
    df['description'] = df['description'].replace('', 'no description available')
    df['repo_names'] = df['repo_names'].replace('', 'no repository name')
    
    print("âœ… í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ì™„ë£Œ")
    # 5. BERT ì„ë² ë”© ìƒì„±
    print("\nğŸ¤– BERT ëª¨ë¸ ë¡œë”© ì¤‘...")
    
    try:
        # ì‚¬ì „ í•™ìŠµëœ BERT ê¸°ë°˜ ëª¨ë¸ ë¡œë“œ (ë‹¤êµ­ì–´ ì§€ì› ëª¨ë¸ë¡œ ë³€ê²½ ê°€ëŠ¥)
        # model = SentenceTransformer('all-MiniLM-L6-v2')  # ë¹ ë¥´ê³  íš¨ê³¼ì ì¸ ëª¨ë¸  #384ì°¨ì›
        model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2') #768ì°¨ì›
        print("âœ… BERT ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
        # Description ì„ë² ë”© ìƒì„±
        print("\nğŸ“ Description ì„ë² ë”© ìƒì„± ì¤‘...")
        description_embeddings = model.encode(
            df['description'].tolist(), 
            show_progress_bar=True,
            batch_size=32,  # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•´ ë°°ì¹˜ í¬ê¸° ì„¤ì •
            convert_to_numpy=True
        )
        
        # Repository names ì„ë² ë”© ìƒì„±
        print("\nğŸ“ Repository names ì„ë² ë”© ìƒì„± ì¤‘...")
        name_embeddings = model.encode(
            df['repo_names'].tolist(), 
            show_progress_bar=True,
            batch_size=32,
            convert_to_numpy=True
        )
        
        print("âœ… BERT ì„ë² ë”© ìƒì„± ì™„ë£Œ")
        
    except Exception as e:
        print(f"âŒ BERT ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
        return
    
    # 6. ì„ë² ë”©ì„ DataFrameìœ¼ë¡œ ë³€í™˜ ë° ê²°í•©
    print("\nğŸ”— ë°ì´í„° ê²°í•© ì¤‘...")
    # Description ì„ë² ë”© DataFrame ìƒì„±
    embedding_df = pd.DataFrame(
        description_embeddings, 
        columns=[f'bert_desc_{i}' for i in range(description_embeddings.shape[1])]
    )
    # Repository names ì„ë² ë”© DataFrame ìƒì„±
    name_df = pd.DataFrame(
        name_embeddings, 
        columns=[f'bert_name_{i}' for i in range(name_embeddings.shape[1])]
    )
    
    # ê¸°ì¡´ DataFrameê³¼ ê²°í•©
    df = pd.concat([df.reset_index(drop=True), name_df, embedding_df], axis=1)
    
    print("âœ… ë°ì´í„° ê²°í•© ì™„ë£Œ")
    print(f"ğŸ“Š ìµœì¢… DataFrame í¬ê¸°: {df.shape}")
    
    # 7. ê²°ê³¼ ì €ì¥
    print("\nğŸ’¾ ê²°ê³¼ ì €ì¥ ì¤‘...")
    
    # ì €ì¥ ê²½ë¡œ ì„¤ì •
    output_dir = 'C:/Users/jun01/OneDrive/ë°”íƒ• í™”ë©´/ê³ ë ¤ëŒ€/ë°ê³¼/TermProject/pkl_data'
    os.makedirs(output_dir, exist_ok=True)
    
    pickle_path = os.path.join(output_dir, 'github_profiles_with_bert_processed_v2.pkl')
    csv_path = os.path.join(output_dir, 'github_profiles_with_bert_processed.csv')
    
    try:
        # í”¼í´ í˜•ì‹ìœ¼ë¡œ ì €ì¥ (ì„ë² ë”© ë°ì´í„° ë³´ì¡´)
        df.to_pickle(pickle_path)
        print(f"âœ… í”¼í´ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {pickle_path}")
        
        '''# CSV í˜•ì‹ìœ¼ë¡œë„ ì €ì¥ (ì¼ë¶€ ì»¬ëŸ¼ë§Œ)
        # BERT ì„ë² ë”©ì€ ë„ˆë¬´ í¬ë¯€ë¡œ ì›ë³¸ ë°ì´í„°ì™€ ì–¸ì–´ í†µê³„ë§Œ ì €ì¥
        basic_columns = [col for col in df.columns if not col.startswith('bert_')]
        df[basic_columns].to_csv(csv_path, index=False, encoding='utf-8')
        print(f"âœ… CSV íŒŒì¼ ì €ì¥ ì™„ë£Œ: {csv_path}")'''
        # Stack ì •ë³´ë„ ë³„ë„ë¡œ ì €ì¥ (ì˜ˆì¸¡ ê³¼ì •ì—ì„œ í™œìš©)
    except Exception as e:
        print(f"âŒ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")
        return
    
    # 8. ê²°ê³¼ ìš”ì•½
    print("\nğŸ“ˆ ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½:")
    print(f"â€¢ ì´ ì‚¬ìš©ì ìˆ˜: {len(df)}")
    print(f"â€¢ ì–¸ì–´ ì»¬ëŸ¼ ìˆ˜: {len(language_columns)}")
    print(f"â€¢ Description ì„ë² ë”© ì°¨ì›: {description_embeddings.shape[1]}")
    print(f"â€¢ Repository names ì„ë² ë”© ì°¨ì›: {name_embeddings.shape[1]}")
    print(f"â€¢ ìµœì¢… í”¼ì²˜ ìˆ˜: {df.shape[1]}")
    
    # ìƒ˜í”Œ ë°ì´í„° í™•ì¸
    print(f"\nğŸ“‹ ìµœì¢… ë°ì´í„° ìƒ˜í”Œ:")
    display_columns = ['username', 'repo_count'] + language_columns[:3] + ['repo_names', 'description']
    available_columns = [col for col in display_columns if col in df.columns]
    print(df[available_columns].head())
    
    print("\nğŸ‰ ì „ì²˜ë¦¬ ì™„ë£Œ!")
    # 1. X êµ¬ì„±
    X_lang = df[language_columns].values.astype(np.float32)
    X_desc = df[[col for col in df.columns if col.startswith('bert_desc_')]].values
    X_name = df[[col for col in df.columns if col.startswith('bert_name_')]].values
    X_total = np.concatenate([X_lang, X_name, X_desc], axis=1).astype(np.float32)

    # 2. y êµ¬ì„±
    le = LabelEncoder()
    y_idx = le.fit_transform(df['stack'])  # ë˜ëŠ” ê°€ì¥ ëŒ€í‘œì ì¸ stackë§Œ ê³ ë¥´ê±°ë‚˜ ì²˜ë¦¬ ë°©ì‹ ì¡°ì •
    def make_onehot(labels, num_classes=None):
        labels = np.array(labels)
        if num_classes is None:
            num_classes = labels.max() + 1
        onehot = np.zeros((len(labels), num_classes), dtype=np.float32)
        onehot[np.arange(len(labels)), labels] = 1.0
        return onehot
    y_onehot = make_onehot(y_idx)

    # íƒ€ê²Ÿ ìŠ¤íƒ ì •ì˜
    target_stacks = ["Android", "Frontend", "ML-Data", "Server", "System", "Visualization", "iOS"]
    
    # stack_listê°€ í¬í•¨ëœ í–‰ ì¤‘, target ìŠ¤íƒë§Œ í¬í•¨ëœ ë¦¬ìŠ¤íŠ¸ ë§Œë“¤ê¸°
    df['filtered_stack'] = df['stack_list'].apply(lambda lst: [s for s in lst if s in target_stacks])
    valid_mask = df['filtered_stack'].apply(lambda x: len(x) > 0)
    print(len(df['filtered_stack']),len(df['stack_list']))
    # Xì™€ y í•„í„°ë§
    X_filtered = X_total[valid_mask.to_numpy()]
    filtered_stack_lists = df.loc[valid_mask, 'filtered_stack'].tolist()
    
    # ì›-í•« ì¸ì½”ë”©
    mlb = MultiLabelBinarizer(classes=target_stacks)
    y_filtered = mlb.fit_transform(filtered_stack_lists)
    
    # ì €ì¥
    np.save(os.path.join(output_dir, "X_filtered.npy"), X_filtered)
    np.save(os.path.join(output_dir, "y_filtered.npy"), y_filtered)
    
    print("âœ… í•„í„°ë§ëœ X/y ì €ì¥ ì™„ë£Œ!")
    print(f"X_filtered shape: {X_filtered.shape}")
    print(f"y_filtered shape: {y_filtered.shape}")
if __name__ == "__main__":
    main()