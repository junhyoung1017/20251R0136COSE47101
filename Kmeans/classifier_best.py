
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, hamming_loss, jaccard_score
from sklearn.multioutput import MultiOutputClassifier
from sklearn.feature_selection import SelectKBest, f_classif
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import warnings
import pickle
import os
warnings.filterwarnings('ignore')

# í•œê¸€ í°íŠ¸ ì„¤ì • (matplotlib)
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False

def load_preprocessed_data(data_dir):
    """
    ì „ì²˜ë¦¬ëœ numpy ë°ì´í„°ë¥¼ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜
    """
    print("ğŸ“‚ ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë”© ì¤‘...")
    
    # ë°ì´í„° íŒŒì¼ ê²½ë¡œ
    X_train_path = os.path.join(data_dir, "X_train_balanced.npy")
    X_test_path = os.path.join(data_dir, "X_test_balanced.npy") 
    y_train_path = os.path.join(data_dir, "y_train_balanced.npy")
    y_test_path = os.path.join(data_dir, "y_test_balanced.npy")
    metadata_path = os.path.join(data_dir, "metadata_enhanced.pkl")
    class_weights_path = os.path.join(data_dir, "class_weights.pkl")
    
    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    files_to_check = [X_train_path, X_test_path, y_train_path, y_test_path, metadata_path]
    for file_path in files_to_check:
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
    
    # ë°ì´í„° ë¡œë“œ
    X_train = np.load(X_train_path)
    X_test = np.load(X_test_path)
    y_train = np.load(y_train_path)
    y_test = np.load(y_test_path)
    
    # ë©”íƒ€ë°ì´í„° ë¡œë“œ
    with open(metadata_path, 'rb') as f:
        metadata = pickle.load(f)
    
    # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ë¡œë“œ (ìˆëŠ” ê²½ìš°)
    class_weights = None
    if os.path.exists(class_weights_path):
        with open(class_weights_path, 'rb') as f:
            class_weights = pickle.load(f)
    
    print(f"âœ… ë°ì´í„° ë¡œë”© ì™„ë£Œ!")
    print(f"ğŸ“Š ë°ì´í„° í¬ê¸°:")
    print(f"   â€¢ í›ˆë ¨ ë°ì´í„°: X={X_train.shape}, y={y_train.shape}")
    print(f"   â€¢ í…ŒìŠ¤íŠ¸ ë°ì´í„°: X={X_test.shape}, y={y_test.shape}")
    print(f"   â€¢ íƒ€ê²Ÿ ìŠ¤íƒ: {metadata['target_stacks']}")
    print(f"   â€¢ ì´ íŠ¹ì„± ìˆ˜: {metadata['total_features']}")
    
    # í´ë˜ìŠ¤ ë¶„í¬ í™•ì¸
    print(f"\nğŸ“Š íƒ€ê²Ÿ ìŠ¤íƒë³„ ë¶„í¬ (í›ˆë ¨ ì„¸íŠ¸):")
    for i, stack in enumerate(metadata['target_stacks']):
        count = np.sum(y_train[:, i])
        percentage = (count / len(y_train)) * 100
        print(f"   {stack}: {count}ê°œ ({percentage:.1f}%)")
    
    return X_train, X_test, y_train, y_test, metadata, class_weights

def feature_analysis(X_train, metadata):
    """
    íŠ¹ì„± ë¶„ì„ ë° ì •ë³´ ì¶œë ¥
    """
    print("\nğŸ” íŠ¹ì„± ë¶„ì„ ì¤‘...")
    
    # íŠ¹ì„± íƒ€ì…ë³„ ë¶„ì„
    language_features = len(metadata.get('language_features', []))
    text_features = len(metadata.get('text_features', []))
    embedding_dims = metadata.get('embedding_dims', {})
    
    print(f"ğŸ“Š íŠ¹ì„± êµ¬ì„±:")
    print(f"   â€¢ ì–¸ì–´ íŠ¹ì„±: {language_features}ê°œ")
    print(f"   â€¢ í…ìŠ¤íŠ¸ íŠ¹ì„±: {text_features}ê°œ")
    print(f"   â€¢ Repository ì´ë¦„ ì„ë² ë”©: {embedding_dims.get('repo_names', 0)}ì°¨ì›")
    print(f"   â€¢ Description ì„ë² ë”©: {embedding_dims.get('description', 0)}ì°¨ì›")
    
    # ê¸°ë³¸ í†µê³„
    print(f"\nğŸ“ˆ íŠ¹ì„± í†µê³„:")
    print(f"   â€¢ í‰ê· : {X_train.mean():.4f}")
    print(f"   â€¢ í‘œì¤€í¸ì°¨: {X_train.std():.4f}")
    print(f"   â€¢ ìµœì†Ÿê°’: {X_train.min():.4f}")
    print(f"   â€¢ ìµœëŒ“ê°’: {X_train.max():.4f}")
    
    # ê²°ì¸¡ì¹˜ í™•ì¸
    nan_count = np.isnan(X_train).sum()
    if nan_count > 0:
        print(f"âš ï¸ ê²°ì¸¡ì¹˜ ë°œê²¬: {nan_count}ê°œ")
        return False
    else:
        print("âœ… ê²°ì¸¡ì¹˜ ì—†ìŒ")
        return True

def prepare_multilabel_models(class_weights=None):
    """
    ë©€í‹°ë¼ë²¨ ë¶„ë¥˜ë¥¼ ìœ„í•œ ëª¨ë¸ë“¤ì„ ì¤€ë¹„í•˜ëŠ” í•¨ìˆ˜
    """
    print("\nğŸ¤– ë©€í‹°ë¼ë²¨ ë¶„ë¥˜ ëª¨ë¸ ì¤€ë¹„ ì¤‘...")
    
    # ê¸°ë³¸ ë¶„ë¥˜ê¸°ë“¤
    base_classifiers = {
        'Random Forest': RandomForestClassifier(
            n_estimators=100, 
            random_state=42, 
            n_jobs=-1,
            class_weight='balanced'
        ),
        'Gradient Boosting': GradientBoostingClassifier(
            n_estimators=100, 
            random_state=42
        ),
        'Logistic Regression': LogisticRegression(
            random_state=42, 
            max_iter=1000,
            class_weight='balanced'
        ),
        'SVM': SVC(
            random_state=42, 
            probability=True,
            class_weight='balanced'
        )
    }
    
    # ë©€í‹°ë¼ë²¨ ë¶„ë¥˜ê¸°ë¡œ ë˜í•‘
    multilabel_models = {}
    for name, classifier in base_classifiers.items():
        multilabel_models[name] = MultiOutputClassifier(classifier, n_jobs=-1)
    
    print(f"âœ… {len(multilabel_models)}ê°œ ë©€í‹°ë¼ë²¨ ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ")
    return multilabel_models

def multilabel_cross_validation(model, X, y, cv=3):
    """
    ë©€í‹°ë¼ë²¨ ë¶„ë¥˜ë¥¼ ìœ„í•œ êµì°¨ ê²€ì¦ (í‰ê·  ë¼ë²¨ ì •í™•ë„ ê¸°ì¤€)
    """
    from sklearn.model_selection import KFold
    
    kf = KFold(n_splits=cv, shuffle=True, random_state=42)
    scores = []
    
    for train_idx, val_idx in kf.split(X):
        X_train_cv, X_val_cv = X[train_idx], X[val_idx]
        y_train_cv, y_val_cv = y[train_idx], y[val_idx]
        
        model.fit(X_train_cv, y_train_cv)
        y_pred_cv = model.predict(X_val_cv)
        
        # ê° ë¼ë²¨ë³„ ì •í™•ë„ ê³„ì‚° í›„ í‰ê· 
        label_accuracies = []
        for i in range(y_val_cv.shape[1]):
            acc = accuracy_score(y_val_cv[:, i], y_pred_cv[:, i])
            label_accuracies.append(acc)
        
        mean_label_accuracy = np.mean(label_accuracies)
        scores.append(mean_label_accuracy)
    
    return np.array(scores)

def calculate_exact_match_accuracy(y_true, y_pred):
    """ì •í™•í•œ ë§¤ì¹­ ì •í™•ë„ ê³„ì‚° (ëª¨ë“  ë¼ë²¨ì´ ì •í™•í•´ì•¼ í•¨)"""
    return np.mean(np.all(y_pred == y_true, axis=1))

def calculate_topk_accuracy(y_true, y_pred_proba, k_values=[1, 2, 3]):
    """Top-k ì •í™•ë„ ê³„ì‚°"""
    if y_pred_proba is None:
        return {k: 0.0 for k in k_values}
    
    results = {}
    for k in k_values:
        # ê° ìƒ˜í”Œì— ëŒ€í•´ ìƒìœ„ kê°œ ì˜ˆì¸¡
        if len(y_pred_proba) > 0 and hasattr(y_pred_proba[0], '__len__'):
            # MultiOutputClassifierì˜ ê²½ìš° ê° ë¶„ë¥˜ê¸°ë³„ í™•ë¥ ì„ í•©ì¹˜ê¸°
            combined_proba = np.zeros((len(y_true), y_true.shape[1]))
            for i in range(y_true.shape[1]):
                combined_proba[:, i] = y_pred_proba[i][:, 1]  # positive class í™•ë¥ 
            y_pred_proba_final = combined_proba
        else:
            y_pred_proba_final = y_pred_proba
        
        top_k_preds = np.argsort(y_pred_proba_final, axis=1)[:, -k:]
        
        # ì‹¤ì œ ë¼ë²¨ê³¼ ë¹„êµ
        matches = []
        for i in range(len(y_true)):
            true_labels = set(np.where(y_true[i] == 1)[0])
            pred_labels = set(top_k_preds[i])
            # êµì§‘í•©ì´ ìˆìœ¼ë©´ ì„±ê³µ
            matches.append(len(true_labels & pred_labels) > 0)
        
        accuracy = np.mean(matches)
        results[k] = accuracy
    
    return results

def train_multilabel_models(X_train, y_train, X_test, y_test, models, target_stacks):
    """
    ë©€í‹°ë¼ë²¨ ëª¨ë¸ë“¤ì„ í•™ìŠµí•˜ê³  í‰ê°€í•˜ëŠ” í•¨ìˆ˜
    """
    print("\nğŸš€ ë©€í‹°ë¼ë²¨ ëª¨ë¸ í•™ìŠµ ë° í‰ê°€ ì¤‘...")
    
    results = {}
    
    for name, model in models.items():
        print(f"\nğŸ”„ {name} í•™ìŠµ ì¤‘...")
        
        try:
            # ëª¨ë¸ í•™ìŠµ
            model.fit(X_train, y_train)
            
            # ì˜ˆì¸¡
            y_pred = model.predict(X_test)
            y_pred_proba = None
            
            # í™•ë¥  ì˜ˆì¸¡ (ê°€ëŠ¥í•œ ê²½ìš°)
            try:
                y_pred_proba = model.predict_proba(X_test)
            except:
                pass
            
            # === ë”¥ëŸ¬ë‹ ì½”ë“œì™€ ë™ì¼í•œ í‰ê°€ ì§€í‘œ ê³„ì‚° ===
            
            # 1. ì •í™•í•œ ë§¤ì¹­ ì •í™•ë„ (Exact Match)
            exact_match_accuracy = calculate_exact_match_accuracy(y_test, y_pred)
            
            # 2. Top-k ì •í™•ë„
            topk_accuracies = calculate_topk_accuracy(y_test, y_pred_proba, k_values=[1, 2, 3])
            
            # 3. Jaccard Score (ë”¥ëŸ¬ë‹ ì½”ë“œì™€ ë™ì¼)
            jaccard_macro = jaccard_score(y_test, y_pred, average='macro', zero_division=0)
            jaccard_micro = jaccard_score(y_test, y_pred, average='micro', zero_division=0)
            jaccard_samples = jaccard_score(y_test, y_pred, average='samples', zero_division=0)
            
            # 4. Hamming Loss
            hamming = hamming_loss(y_test, y_pred)
            
            # 5. ê° ë¼ë²¨ë³„ ì •í™•ë„ (í‰ê·  ë¼ë²¨ ì •í™•ë„)
            label_accuracies = []
            for i in range(y_test.shape[1]):
                acc = accuracy_score(y_test[:, i], y_pred[:, i])
                label_accuracies.append(acc)
            mean_label_accuracy = np.mean(label_accuracies)
            
            # 6. ë¼ë²¨ë³„ ì„¸ë¶€ ì„±ëŠ¥ (Precision, Recall, F1)
            label_metrics = []
            for i in range(y_test.shape[1]):
                true_positive = np.sum((y_test[:, i] == 1) & (y_pred[:, i] == 1))
                false_positive = np.sum((y_test[:, i] == 0) & (y_pred[:, i] == 1))
                false_negative = np.sum((y_test[:, i] == 1) & (y_pred[:, i] == 0))
                
                precision = true_positive / (true_positive + false_positive) if (true_positive + false_positive) > 0 else 0
                recall = true_positive / (true_positive + false_negative) if (true_positive + false_negative) > 0 else 0
                f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
                
                label_metrics.append({
                    'precision': precision,
                    'recall': recall,
                    'f1': f1
                })
            
            # êµì°¨ ê²€ì¦
            try:
                cv_scores = multilabel_cross_validation(model, X_train, y_train, cv=3)
                cv_mean = cv_scores.mean()
                cv_std = cv_scores.std()
            except Exception as e:
                print(f"   âš ï¸ êµì°¨ ê²€ì¦ ì‹¤íŒ¨: {e}")
                cv_mean, cv_std = 0, 0
            
            # ê²°ê³¼ ì €ì¥
            results[name] = {
                'model': model,
                # ì£¼ìš” ì„±ëŠ¥ ì§€í‘œ (ë”¥ëŸ¬ë‹ ì½”ë“œì™€ ë™ì¼)
                'exact_match_accuracy': exact_match_accuracy,
                'top1_accuracy': topk_accuracies[1],
                'top2_accuracy': topk_accuracies[2],
                'top3_accuracy': topk_accuracies[3],
                'jaccard_macro': jaccard_macro,
                'jaccard_micro': jaccard_micro,
                'jaccard_samples': jaccard_samples,
                'hamming_loss': hamming,
                'mean_label_accuracy': mean_label_accuracy,
                # ì„¸ë¶€ ì •ë³´
                'label_accuracies': label_accuracies,
                'label_metrics': label_metrics,
                'cv_mean': cv_mean,
                'cv_std': cv_std,
                'predictions': y_pred,
                'probabilities': y_pred_proba
            }
            
            print(f"âœ… {name} ì™„ë£Œ!")
            print(f"   ğŸ“Š ì£¼ìš” ì„±ëŠ¥ ì§€í‘œ:")
            print(f"     â€¢ ì •í™•í•œ ë§¤ì¹­: {exact_match_accuracy:.4f} ({exact_match_accuracy:.2%})")
            print(f"     â€¢ Top-1 ì •í™•ë„: {topk_accuracies[1]:.4f} ({topk_accuracies[1]:.2%})")
            print(f"     â€¢ Top-2 ì •í™•ë„: {topk_accuracies[2]:.4f} ({topk_accuracies[2]:.2%})")
            print(f"     â€¢ Jaccard Score: {jaccard_macro:.4f}")
            print(f"     â€¢ í‰ê·  ë¼ë²¨ ì •í™•ë„: {mean_label_accuracy:.4f}")
            print(f"     â€¢ êµì°¨ ê²€ì¦: {cv_mean:.4f} (Â±{cv_std:.4f})")
            
        except Exception as e:
            print(f"âŒ {name} í•™ìŠµ ì‹¤íŒ¨: {e}")
            continue
    
    return results

def evaluate_multilabel_results(results, y_test, target_stacks):
    """
    ë©€í‹°ë¼ë²¨ ë¶„ë¥˜ ê²°ê³¼ë¥¼ í‰ê°€í•˜ê³  ìµœê³  ëª¨ë¸ì„ ì„ íƒí•˜ëŠ” í•¨ìˆ˜ (ë”¥ëŸ¬ë‹ ì½”ë“œì™€ ë™ì¼í•œ ì§€í‘œ)
    """
    print("\nğŸ† ë©€í‹°ë¼ë²¨ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ:")
    print("-" * 100)
    print(f"{'Model':<20} | {'Exact Match':<12} | {'Top-1':<8} | {'Top-2':<8} | {'Jaccard':<8} | {'Mean Acc':<8}")
    print("-" * 100)
    
    # ì„±ëŠ¥ ë¹„êµ ì¶œë ¥
    best_score = 0
    best_model_name = None
    
    for name, result in results.items():
        exact_match = result['exact_match_accuracy']
        top1 = result['top1_accuracy']
        top2 = result['top2_accuracy']
        jaccard = result['jaccard_macro']
        mean_acc = result['mean_label_accuracy']
        
        print(f"{name:<20} | {exact_match:<12.4f} | {top1:<8.4f} | {top2:<8.4f} | {jaccard:<8.4f} | {mean_acc:<8.4f}")
        
        # Top-1 ì •í™•ë„ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ìµœê³  ëª¨ë¸ ì„ íƒ (ë”¥ëŸ¬ë‹ê³¼ ë¹„êµí•˜ê¸° ìœ„í•´)
        if top1 > best_score:
            best_score = top1
            best_model_name = name
    
    print("-" * 100)
    print(f"\nğŸ¥‡ ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model_name}")
    print(f"   Top-1 ì •í™•ë„: {best_score:.4f} ({best_score:.2%})")
    
    # ìµœê³  ëª¨ë¸ì˜ ìƒì„¸ í‰ê°€
    best_result = results[best_model_name]
    
    print(f"\nğŸ“Š {best_model_name} ìƒì„¸ í‰ê°€:")
    print(f"   ğŸ¯ ì£¼ìš” ì„±ëŠ¥ ì§€í‘œ:")
    print(f"     â€¢ ì •í™•í•œ ë§¤ì¹­ (Exact Match): {best_result['exact_match_accuracy']:.4f} ({best_result['exact_match_accuracy']:.2%})")
    print(f"     â€¢ Top-1 ì •í™•ë„: {best_result['top1_accuracy']:.4f} ({best_result['top1_accuracy']:.2%})")
    print(f"     â€¢ Top-2 ì •í™•ë„: {best_result['top2_accuracy']:.4f} ({best_result['top2_accuracy']:.2%})")
    print(f"     â€¢ Top-3 ì •í™•ë„: {best_result['top3_accuracy']:.4f} ({best_result['top3_accuracy']:.2%})")
    print(f"   ğŸ“ˆ ì¶”ê°€ ì§€í‘œ:")
    print(f"     â€¢ Jaccard Score (Macro): {best_result['jaccard_macro']:.4f}")
    print(f"     â€¢ Jaccard Score (Micro): {best_result['jaccard_micro']:.4f}")
    print(f"     â€¢ Hamming Loss: {best_result['hamming_loss']:.4f}")
    print(f"     â€¢ í‰ê·  ë¼ë²¨ ì •í™•ë„: {best_result['mean_label_accuracy']:.4f}")
    
    # ê° ë¼ë²¨ë³„ ì„±ëŠ¥
    print(f"\nğŸ“‹ ìŠ¤íƒë³„ ì„±ëŠ¥:")
    for i, (stack, metrics) in enumerate(zip(target_stacks, best_result['label_metrics'])):
        acc = best_result['label_accuracies'][i]
        precision = metrics['precision']
        recall = metrics['recall']
        f1 = metrics['f1']
        print(f"   {stack:>15}: Acc={acc:.3f}, P={precision:.3f}, R={recall:.3f}, F1={f1:.3f}")
    
    return best_result['model'], best_model_name, best_result

def visualize_multilabel_results(results, y_test, target_stacks, best_model_name):
    """
    ë©€í‹°ë¼ë²¨ ë¶„ë¥˜ ê²°ê³¼ë¥¼ ì‹œê°í™”í•˜ëŠ” í•¨ìˆ˜ (ë”¥ëŸ¬ë‹ ì½”ë“œì™€ ë™ì¼í•œ ì§€í‘œ í¬í•¨)
    """
    print("\nğŸ“ˆ ê²°ê³¼ ì‹œê°í™” ì¤‘...")
    
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    
    # 1. ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ (Top-1 ì •í™•ë„)
    model_names = list(results.keys())
    top1_scores = [results[name]['top1_accuracy'] for name in model_names]
    exact_match_scores = [results[name]['exact_match_accuracy'] for name in model_names]
    
    axes[0, 0].bar(model_names, top1_scores, alpha=0.7, color='skyblue', label='Top-1')
    axes[0, 0].bar(model_names, exact_match_scores, alpha=0.7, color='lightcoral', label='Exact Match')
    axes[0, 0].set_title('Model Performance (Top-1 vs Exact Match)')
    axes[0, 0].set_ylabel('Accuracy')
    axes[0, 0].tick_params(axis='x', rotation=45)
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)
    
    # 2. Top-k ì •í™•ë„ ë¹„êµ (ìµœê³  ëª¨ë¸)
    best_result = results[best_model_name]
    topk_values = [best_result['top1_accuracy'], best_result['top2_accuracy'], best_result['top3_accuracy']]
    topk_labels = ['Top-1', 'Top-2', 'Top-3']
    
    axes[0, 1].bar(topk_labels, topk_values, alpha=0.7, color='lightgreen')
    axes[0, 1].set_title(f'{best_model_name} - Top-k Accuracy')
    axes[0, 1].set_ylabel('Accuracy')
    axes[0, 1].grid(True, alpha=0.3)
    for i, v in enumerate(topk_values):
        axes[0, 1].text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')
    
    # 3. Jaccard Score ë¹„êµ
    jaccard_scores = [results[name]['jaccard_macro'] for name in model_names]
    
    axes[0, 2].bar(model_names, jaccard_scores, alpha=0.7, color='gold')
    axes[0, 2].set_title('Model Performance (Jaccard Score)')
    axes[0, 2].set_ylabel('Jaccard Score')
    axes[0, 2].tick_params(axis='x', rotation=45)
    axes[0, 2].grid(True, alpha=0.3)
    
    # 4. ìµœê³  ëª¨ë¸ì˜ ë¼ë²¨ë³„ ì •í™•ë„
    label_accuracies = best_result['label_accuracies']
    
    axes[1, 0].bar(target_stacks, label_accuracies, alpha=0.7, color='plum')
    axes[1, 0].set_title(f'{best_model_name} - Label-wise Accuracy')
    axes[1, 0].set_ylabel('Accuracy')
    axes[1, 0].tick_params(axis='x', rotation=45)
    axes[1, 0].grid(True, alpha=0.3)
    
    # 5. ì‹¤ì œ vs ì˜ˆì¸¡ ë¼ë²¨ ë¶„í¬
    y_pred = best_result['predictions']
    
    true_counts = np.sum(y_test, axis=0)
    pred_counts = np.sum(y_pred, axis=0)
    
    x = np.arange(len(target_stacks))
    width = 0.35
    
    axes[1, 1].bar(x - width/2, true_counts, width, label='True', alpha=0.7, color='steelblue')
    axes[1, 1].bar(x + width/2, pred_counts, width, label='Predicted', alpha=0.7, color='orange')
    axes[1, 1].set_title('True vs Predicted Label Distribution')
    axes[1, 1].set_ylabel('Count')
    axes[1, 1].set_xticks(x)
    axes[1, 1].set_xticklabels(target_stacks, rotation=45)
    axes[1, 1].legend()
    axes[1, 1].grid(True, alpha=0.3)
    
    # 6. ì„±ëŠ¥ ì§€í‘œ ìš”ì•½ (ë ˆì´ë” ì°¨íŠ¸ ëŒ€ì‹  ë°” ì°¨íŠ¸)
    metrics_names = ['Exact Match', 'Top-1', 'Top-2', 'Jaccard', 'Mean Label Acc']
    metrics_values = [
        best_result['exact_match_accuracy'],
        best_result['top1_accuracy'], 
        best_result['top2_accuracy'],
        best_result['jaccard_macro'],
        best_result['mean_label_accuracy']
    ]
    
    axes[1, 2].barh(metrics_names, metrics_values, alpha=0.7, color='lightsteelblue')
    axes[1, 2].set_title(f'{best_model_name} - Performance Summary')
    axes[1, 2].set_xlabel('Score')
    axes[1, 2].grid(True, alpha=0.3)
    for i, v in enumerate(metrics_values):
        axes[1, 2].text(v + 0.01, i, f'{v:.3f}', ha='left', va='center')
    
    plt.tight_layout()
    plt.show()
    
    # ì¶”ê°€: ë¼ë²¨ë³„ í˜¼ë™ í–‰ë ¬ ì‹œê°í™”
    print("\nğŸ“Š ë¼ë²¨ë³„ í˜¼ë™ í–‰ë ¬:")
    fig, axes = plt.subplots(2, 4, figsize=(20, 10))
    axes = axes.flatten()
    
    for i, stack in enumerate(target_stacks):
        cm = confusion_matrix(y_test[:, i], y_pred[:, i])
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[i])
        axes[i].set_title(f'{stack}')
        axes[i].set_xlabel('Predicted')
        axes[i].set_ylabel('True')
    
    # ë§ˆì§€ë§‰ ì¶• ìˆ¨ê¸°ê¸°
    axes[-1].axis('off')
    
    plt.tight_layout()
    plt.show()

def save_best_model(best_model, best_model_name, metadata, data_dir):
    """
    ìµœê³  ì„±ëŠ¥ ëª¨ë¸ì„ ì €ì¥í•˜ëŠ” í•¨ìˆ˜
    """
    print(f"\nğŸ’¾ ìµœê³  ëª¨ë¸ ì €ì¥ ì¤‘...")
    
    model_save_path = os.path.join(data_dir, "best_multilabel_model.pkl")
    
    model_data = {
        'model': best_model,
        'model_name': best_model_name,
        'target_stacks': metadata['target_stacks'],
        'metadata': metadata,
        'model_type': 'multilabel'
    }
    
    with open(model_save_path, 'wb') as f:
        pickle.dump(model_data, f)
    
    print(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_save_path}")
    return model_save_path

def main():
    # ë°ì´í„° ê²½ë¡œ ì„¤ì •
    data_dir = 'C:/Users/jun01/OneDrive/ë°”íƒ• í™”ë©´/ê³ ë ¤ëŒ€/ë°ê³¼/TermProject/pkl_data'
    
    try:
        # 1. ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë“œ
        X_train, X_test, y_train, y_test, metadata, class_weights = load_preprocessed_data(data_dir)
        
        # 2. íŠ¹ì„± ë¶„ì„
        is_valid = feature_analysis(X_train, metadata)
        if not is_valid:
            print("âŒ ë°ì´í„°ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ì „ì²˜ë¦¬ë¥¼ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”.")
            return
        
        # 3. ë©€í‹°ë¼ë²¨ ëª¨ë¸ ì¤€ë¹„
        models = prepare_multilabel_models(class_weights)
        
        # 4. ìŠ¤ì¼€ì¼ë§ (ì´ë¯¸ ì „ì²˜ë¦¬ì—ì„œ ì¼ë¶€ ì²˜ë¦¬ë˜ì—ˆì§€ë§Œ, ì¶”ê°€ ì •ê·œí™”)
        print("\nâš–ï¸ íŠ¹ì„± ìŠ¤ì¼€ì¼ë§ ì¤‘...")
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        print(f"âœ… ìŠ¤ì¼€ì¼ë§ ì™„ë£Œ")
        
        # 5. ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
        results = train_multilabel_models(
            X_train_scaled, y_train, X_test_scaled, y_test, 
            models, metadata['target_stacks']
        )
        
        if not results:
            print("âŒ ëª¨ë“  ëª¨ë¸ í•™ìŠµì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            return
        
        # 6. ê²°ê³¼ í‰ê°€ ë° ìµœê³  ëª¨ë¸ ì„ íƒ
        best_model, best_model_name, best_result = evaluate_multilabel_results(
            results, y_test, metadata['target_stacks']
        )
        
        # 7. ê²°ê³¼ ì‹œê°í™”
        visualize_multilabel_results(
            results, y_test, metadata['target_stacks'], best_model_name
        )
        
        # 8. ìµœê³  ëª¨ë¸ ì €ì¥
        model_save_path = save_best_model(best_model, best_model_name, metadata, data_dir)
        
        # 9. ìµœì¢… ìš”ì•½
        print(f"\nğŸ‰ ë©€í‹°ë¼ë²¨ ìŠ¤íƒ ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")
        print(f"ğŸ“Š ìµœì¢… ê²°ê³¼ ìš”ì•½:")
        print(f"   â€¢ ìµœê³  ëª¨ë¸: {best_model_name}")
        print(f"   ğŸ¯ ì£¼ìš” ì„±ëŠ¥ ì§€í‘œ (ë”¥ëŸ¬ë‹ê³¼ ë¹„êµìš©):")
        print(f"     â€¢ ì •í™•í•œ ë§¤ì¹­ (Exact Match): {best_result['exact_match_accuracy']:.4f} ({best_result['exact_match_accuracy']:.2%})")
        print(f"     â€¢ Top-1 ì •í™•ë„: {best_result['top1_accuracy']:.4f} ({best_result['top1_accuracy']:.2%})")
        print(f"     â€¢ Top-2 ì •í™•ë„: {best_result['top2_accuracy']:.4f} ({best_result['top2_accuracy']:.2%})")
        print(f"     â€¢ Top-3 ì •í™•ë„: {best_result['top3_accuracy']:.4f} ({best_result['top3_accuracy']:.2%})")
        print(f"     â€¢ Jaccard Score: {best_result['jaccard_macro']:.4f}")
        print(f"     â€¢ í‰ê·  ë¼ë²¨ ì •í™•ë„: {best_result['mean_label_accuracy']:.4f}")
        print(f"   â€¢ ì €ì¥ ê²½ë¡œ: {model_save_path}")
        
        # ì„±ëŠ¥ ë¹„êµ ê°€ì´ë“œ
        print(f"\nğŸ“ˆ ë”¥ëŸ¬ë‹ ëª¨ë¸ê³¼ ì„±ëŠ¥ ë¹„êµ:")
        print(f"   ì´ ê²°ê³¼ë¥¼ ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ ë‹¤ìŒ ì§€í‘œì™€ ë¹„êµí•˜ì„¸ìš”:")
        print(f"   - ì •í™•í•œ ë§¤ì¹­ ë¹„ìœ¨ (exact_match_ratio)")
        print(f"   - Top-1, Top-2, Top-3 ì •í™•ë„")
        print(f"   - Jaccard Score (Macro)")
        
        # ì‚¬ìš© ê°€ì´ë“œ
        print(f"\nğŸ“– ëª¨ë¸ ì‚¬ìš© ê°€ì´ë“œ:")
        print(f"import pickle")
        print(f"with open('{model_save_path}', 'rb') as f:")
        print(f"    model_data = pickle.load(f)")
        print(f"model = model_data['model']")
        print(f"predictions = model.predict(new_X)")
        print(f"# í™•ë¥  ì˜ˆì¸¡:")
        print(f"probabilities = model.predict_proba(new_X)")
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()