import numpy as np
import pandas as pd
from sentence_transformers import SentenceTransformer
import pickle
import re
from typing import Tuple, List
import os
from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.feature_selection import VarianceThreshold
from sklearn.utils.class_weight import compute_class_weight
from sklearn.model_selection import train_test_split
from collections import Counter
import warnings
warnings.filterwarnings('ignore')

# imbalanced-learn ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ í•„ìš”: pip install imbalanced-learn
try:
    from imblearn.over_sampling import SMOTE, ADASYN
    from imblearn.combine import SMOTETomek
    IMBLEARN_AVAILABLE = True
except ImportError:
    print("âš ï¸ imbalanced-learnì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install imbalanced-learnë¡œ ì„¤ì¹˜í•˜ì„¸ìš”.")
    print("   í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ë°©ë²•ë§Œ ì‚¬ìš©ë©ë‹ˆë‹¤.")
    IMBLEARN_AVAILABLE = False

# íŒŒì¼ ê²½ë¡œ
file_path = 'C:/Users/jun01/OneDrive/ë°”íƒ• í™”ë©´/ê³ ë ¤ëŒ€/ë°ê³¼/TermProject/20251R0136COSE47101/Kmeans/github_profiles_total_v5.csv'

def split_repos(text: str) -> Tuple[str, str]:
    """
    Repository í…ìŠ¤íŠ¸ë¥¼ ì´ë¦„ê³¼ ì„¤ëª…ìœ¼ë¡œ ë¶„ë¦¬í•˜ëŠ” í•¨ìˆ˜
    """
    if pd.isna(text) or text == '':
        return '', ''
    
    repos = str(text).split('/')  # ê° repo êµ¬ë¶„
    repo_names = []
    descriptions = []
    
    for repo in repos:
        parts = repo.split('::')
        name = parts[0].strip()
        desc = parts[1].strip() if len(parts) > 1 else ''
        
        # ë¹ˆ ë¬¸ìì—´ì´ ì•„ë‹Œ ê²½ìš°ë§Œ ì¶”ê°€
        if name and name.lower() not in ['nan', 'none', '']:
            repo_names.append(name)
        if desc and desc.lower() not in ['nan', 'none', '']:
            descriptions.append(desc)
    
    return ', '.join(repo_names), ', '.join(descriptions)

def process_stack(stack_text: str) -> List[str]:
    """
    Stack í…ìŠ¤íŠ¸ë¥¼ &ìœ¼ë¡œ ë¶„ë¦¬í•˜ê³  ì •ì œí•˜ëŠ” í•¨ìˆ˜ (ë©€í‹°ìŠ¤íƒ ì²˜ë¦¬)
    """
    if pd.isna(stack_text) or stack_text == '':
        return []
    
    # &ìœ¼ë¡œ ë¶„ë¦¬í•˜ê³  ê° ìŠ¤íƒ ì •ì œ
    stacks = [s.strip() for s in str(stack_text).split('&') if s.strip()]
    
    # ë¹ˆ ë¬¸ìì—´ì´ë‚˜ 'nan' ì œê±°, ê³µë°± ì •ë¦¬
    cleaned_stacks = []
    for stack in stacks:
        # ê³µë°± ì •ë¦¬ (Frontend& Server -> Frontend, Server)
        stack = stack.strip()
        if stack and stack.lower() not in ['nan', 'none', '']:
            cleaned_stacks.append(stack)
    
    return cleaned_stacks

def clean_text(text: str) -> str:
    """
    í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í•¨ìˆ˜
    """
    if pd.isna(text) or text == '':
        return ''
    
    # ì†Œë¬¸ì ë³€í™˜
    text = str(text).lower()
    
    # URL ì œê±°
    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)
    
    # ì´ë©”ì¼ ì œê±°
    text = re.sub(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', '', text)
    
    # íŠ¹ìˆ˜ë¬¸ì ì œê±° (ì˜ë¬¸, ìˆ«ì, ê³µë°±ë§Œ ë‚¨ê¹€)
    text = re.sub(r'[^a-zA-Z0-9\s]', ' ', text)
    
    # ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ
    text = re.sub(r'\s+', ' ', text)
    
    # ì•ë’¤ ê³µë°± ì œê±°
    text = text.strip()
    
    # ë„ˆë¬´ ì§§ì€ í…ìŠ¤íŠ¸ ì²˜ë¦¬
    if len(text) < 3:
        return ''
    
    return text

def create_language_features(df: pd.DataFrame, language_columns: List[str]) -> pd.DataFrame:
    """
    ì–¸ì–´ ë°ì´í„°ë¡œë¶€í„° ì¶”ê°€ íŠ¹ì„±ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
    """
    print("ğŸ”§ ì–¸ì–´ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì¤‘...")
    
    # ì–¸ì–´ ë°ì´í„°ë¥¼ ìˆ«ìí˜•ìœ¼ë¡œ ë³€í™˜
    for col in language_columns:
        df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)
    
    # 1. ì´ ì½”ë“œ ë¼ì¸ ìˆ˜
    total_lines=df[language_columns].sum(axis=1)
    
    # 2. ì‚¬ìš© ì–¸ì–´ ê°œìˆ˜
    df['num_languages'] = (df[language_columns] > 0).sum(axis=1)
    
    # 3. ì£¼ë ¥ ì–¸ì–´ (ê°€ì¥ ë§ì´ ì‚¬ìš©í•œ ì–¸ì–´)
    df['main_language_idx'] = df[language_columns].idxmax(axis=1)
    df['main_language_ratio'] = df[language_columns].max(axis=1) / (total_lines + 1e-6)
    
    # 4. ì–¸ì–´ ë‹¤ì–‘ì„± ì§€ìˆ˜ (Shannon entropy) - ê°œì„ ëœ ë²„ì „
    def calculate_diversity(row):
        try:
            values = np.array(row[language_columns].values, dtype=np.float64)
            total = np.sum(values)
            if total == 0 or np.isnan(total):
                return 0.0
            
            probs = values / total
            probs = probs[probs > 0]  # 0ì¸ ê°’ ì œê±°
            
            if len(probs) <= 1:  # í•˜ë‚˜ ì´í•˜ì˜ ì–¸ì–´ë§Œ ì‚¬ìš©
                return 0.0
            
            # Shannon entropy ê³„ì‚°
            log_probs = np.log2(probs + 1e-10)  # ë” ì‘ì€ epsilon ì‚¬ìš©
            entropy = -np.sum(probs * log_probs)
            
            # ê²°ê³¼ê°€ ìœ íš¨í•œì§€ í™•ì¸
            if np.isnan(entropy) or np.isinf(entropy):
                return 0.0
            
            return float(entropy)
            
        except Exception as e:
            print(f"Warning: calculate_diversity ì—ëŸ¬ ë°œìƒ: {e}")
            return 0.0
    
    df['language_diversity'] = df.apply(calculate_diversity, axis=1)
    
    # 5. Frontend/Backend/Others ë¹„ìœ¨ (ì‹¤ì œ ì–¸ì–´ ê¸°ë°˜)
    frontend_langs = ['JS'] 
    backend_langs = ['Python', 'Java', 'C++', 'C#', 'Go', 'PHP', 'Ruby']
    mobile_langs = ['Swift', 'Kotlin', 'Dart']
    system_langs = ['C/C++', 'Rust', 'Assembly']
    
    frontend_cols = [col for col in frontend_langs if col in language_columns]
    backend_cols = [col for col in backend_langs if col in language_columns]
    mobile_cols = [col for col in mobile_langs if col in language_columns]
    system_cols = [col for col in system_langs if col in language_columns]
    
    if frontend_cols:
        df['frontend_lang_ratio'] = df[frontend_cols].sum(axis=1) / (total_lines+ 1e-6)
    else:
        df['frontend_lang_ratio'] = 0
        
    if backend_cols:
        df['backend_lang_ratio'] = df[backend_cols].sum(axis=1) / (total_lines + 1e-6)
    else:
        df['backend_lang_ratio'] = 0
        
    if mobile_cols:
        df['mobile_lang_ratio'] = df[mobile_cols].sum(axis=1) / (total_lines + 1e-6)
    else:
        df['mobile_lang_ratio'] = 0
        
    if system_cols:
        df['system_lang_ratio'] = df[system_cols].sum(axis=1) / (total_lines+ 1e-6)
    else:
        df['system_lang_ratio'] = 0
    
    # 6. ì–¸ì–´ë³„ ì •ê·œí™” (Min-Max Scaling)
    scaler = MinMaxScaler()
    df[language_columns] = scaler.fit_transform(df[language_columns])
    
    print("âœ… ì–¸ì–´ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì™„ë£Œ")
    return df
def create_stack_specific_keywords(df: pd.DataFrame) -> pd.DataFrame:
    """
    7ê°œ ìŠ¤íƒë³„ íŠ¹í™” í‚¤ì›Œë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ íŠ¹ì„±ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
    """
    print("ğŸ¯ ìŠ¤íƒë³„ íŠ¹í™” í‚¤ì›Œë“œ íŠ¹ì„± ìƒì„± ì¤‘...")
    
    # ìŠ¤íƒë³„ íŠ¹í™” í‚¤ì›Œë“œ ì •ì˜
    stack_keywords = {
        'frontend': [
            'react', 'angular', 'vue', 'svelte',
            'html', 'css', 'sass', 'typescript', 'javascript',
            'bootstrap', 'tailwind', 'ui', 'ux', 'dom',
            'webpack', 'vite', 'next', 'nuxt',
            'spa', 'pwa', 'website', 'browser', 'frontend'
        ],
        'server':['server', 'backend', 'rest', 'graphql', 'rpc', 'webhook',
            'microservice', 'monolith', 'database', 'sql', 'nosql',
            'mongodb', 'postgresql', 'mysql', 'redis', 'sqlite', 'elastic', 'elastic-search',
            'node.js', 'express', 'koa', 'hapi', 'fastify', 'fastapi', 'django', 'flask',
            'spring', 'laravel', 'nest', 'sanic', 'gin', 'actix', 'routing', 'controller',
            'mvc', 'orm', 'prisma', 'sequelize', 'typeorm','router'
            'authentication', 'authorization', 'jwt', 'oauth', 'session', 'cookie',
            'middleware', 'endpoint', 'http', 'https', 'request', 'response',
            'cache',  'reverse-proxy', 'cors', 'rate-limiting',
            'websocket', 'swagger', 'openapi', 'rabbitmq', 'kafka',  'tomcat', 
            'bcrypt', 'hashing', 'encryption', 'salt', 'socket.io', 'api-gateway',
            'service-mesh', 'circuit-breaker', 'strapi', 'quarkus', 'soa', 'bff', 'postman', 'insomnia'
            'backend-service', 'api-server', 'load-balancer', 'reverse-proxy', 'infrastructure'],
        # ''''server': [
        #     'java', 'data', 'python', 'client', 
        #     'react', 'php', 'javascript', 'system', 'server', 'apache', 'spring', 
        #     'management', 'bootstrap', 'laravel', 'django', 'flask', 'node', 'express',
        #     'sql', 'nosql', 'mongodb', 'postgresql', 'mysql', 'redis',
        #     'docker', 'kubernetes', 'aws', 'azure', 'gcp', 'microservice', 'rest',
        #     'graphql', 'jwt', 'oauth', 'crud', 'orm'
        # ],'''
        # 'server': [
#     'backend', 'rest', 'graphql', 'rpc', 'webhook',
#     'microservice', 'monolith',
#     'api-server', 'controller', 'middleware', 'routing',
#     'express', 'koa', 'hapi', 'fastify', 'nest',
#     'django', 'flask', 'spring', 'laravel',
#     'prisma', 'sequelize', 'typeorm', 'orm',
#     'jwt', 'oauth', 'session', 'cookie', 'authentication', 'authorization',
#     'swagger', 'openapi', 'postman', 'insomnia'
# ]     
        'android': [
            'android', 'flutter', 'native', 'view',
            'design', 'material', 'kotlin', 'support', 
            'image', 'video', 'sdk', 'gradle', 'studio', 'activity', 
            'fragment', 'intent', 'recyclerview', 'room', 'retrofit', 'coroutines',
            'jetpack', 'compose', 'mvvm', 'livedata', 'viewmodel', 'firebase', 'play'
        ],
        
        'ios': [
            'ios', 'swift','library', 'flutter', 'swiftui', 
            'development', 'package', 'custom', 'xamarin', 'objective', 'xcode', 
            'iphone', 'ipad', 'uikit', 'cocoa', 'pods', 'carthage', 'realm',
            'storyboard', 'autolayout', 'delegate', 'protocol', 'arc', 'gcd',
            'appstore', 'testflight'
        ],
        
       'visualization': [
            'data', 'visualization', 'chart', 'graph', 'd3', 'plotly', 'dashboard', 
            'interactive', 'analysis', 'report',
            'matplotlib', 'seaborn', 'bokeh', 'tableau', 'powerbi', 'chart.js',
            'highcharts', 'echarts', 'vis', 'insight', 'metric', 'kpi', 'business',
            'intelligence', 'bi', 'analytics'
        ],
        
        'ml_data': [
            'learning', 'python', 'data', 'deep', 'tensorflow', 'machine', 
            'networks', 'models', 'pytorch', 'neural', 'object', 'caffe', 
            'detection', 'reinforcement', 'analysis', 'ai', 'artificial', 'intelligence',
            'keras', 'sklearn', 'pandas', 'numpy', 'jupyter', 'notebook', 'algorithm',
            'regression', 'classification', 'clustering', 'nlp', 'cv', 'computer', 'vision',
            'feature', 'training', 'prediction', 'dataset', 'preprocessing'
        ],
         'system' : [
                'linux', 'unix', 'windows', 'macos', 'kernel', 'bash', 'shell', 'powershell',
                'infrastructure', 'deployment', 'ci', 'cd', 'build', 'runner', 'system','os','infra'
                'docker', 'kubernetes', 'container', 'pod', 'cluster', 'command-line', 'terminal', 'script', 'crontab',
                'ansible', 'terraform', 'helm', 'logging', 'monitoring', 'prometheus', 'grafana','gunicorn', 
                'nginx', 'pm2', 'gunicorn', 'socket', 'tcp', 'udp', 'dns','loadbalancer', 'proxy',
                'load', 'latency', 'uptime', 'network', 'firewall', 'security','node','nginx','pm2','pipeline', 
                 'automation',  'backup', 'restore','configuration', 'recovery', 'admin', 'failover', 'scalability'
            ],
        #  'system': [
        #     'system', 'linux', 'server', 'network', 'security', 'docker', 'kubernetes', 
        #     'monitoring', 'deployment', 'infrastructure', 'devops', 'automation', 
        #     'performance', 'unix', 'windows', 'shell', 'bash', 'script', 'ci', 'cd',
        #     'pipeline', 'logging', 'admin', 'maintenance', 'backup', 'recovery',
        #     'scalability', 'load', 'balancer', 'configuration'
        # ]
        # 'system' : [
        #         'system', 'linux', 'unix', 'windows', 'macos', 'kernel', 'os',
        #         'bash', 'shell', 'powershell', 'command-line', 'terminal', 'script', 'crontab',
        #         'devops', 'infrastructure', 'infra', 'deployment', 'build', 'ci', 'cd', 
        #         'pipeline', 'runner', 'monitoring', 'prometheus', 'grafana', 'logging',
        #         'logstash', 'log', 'performance', 'profiling', 'security', 'firewall',
        #         'network', 'socket', 'tcp', 'udp', 'dns', 'load', 'latency', 'uptime',
        #         'docker', 'kubernetes', 'swarm', 'cluster', 'container', 'pod', 'node',
        #         'service', 'ingress', 'helm', 'ansible', 'terraform', 'puppet', 'chef',
        #         'automation', 'configuration', 'provisioning', 'backup', 'restore',
        #         'recovery', 'maintenance', 'admin', 'failover', 'scalability'
        #     ],
    }
    
    # ê¸°ì¡´ ì¼ë°˜ í‚¤ì›Œë“œë“¤ (í˜¸í™˜ì„± ìœ ì§€)
    general_tech_keywords = [
        'agile', 'algorithm', 'api', 'app', 'automation', 'aws', 'azure', 'backend',
    'bot', 'build', 'cd', 'ci', 'clean-code', 'client', 'cloud', 'comment',
    'config', 'container', 'cron', 'data', 'database', 'debug', 'deep', 'deploy',
    'design-pattern', 'devops', 'docker', 'documentation', 'feature', 'framework',
    'frontend', 'fullstack', 'game', 'gcp', 'git', 'github', 'gitlab', 'graphql',
    'infrastructure', 'integration', 'issue', 'json', 'kanban', 'kubernetes',
    'learning', 'library', 'log', 'logging', 'machine', 'metrics', 'microservice',
    'mobile', 'mock', 'module', 'monitor', 'neural', 'package', 'performance',
    'pipeline', 'provisioning', 'pull-request', 'readme', 'refactor',
    'reliability', 'rest', 'review', 'run', 'scalability', 'script', 'scrum',
    'sdk', 'server', 'sprint', 'test', 'ticket', 'tool', 'ui', 'unit-test', 'ux',
    'version', 'web', 'xml', 'yaml','javascript'
    ]
    # general_tech_keywords = [
    #     'api', 'web', 'app', 'mobile', 'data', 'machine', 'learning', 
    #     'deep', 'neural', 'algorithm', 'database', 'server', 'client',
    #     'framework', 'library', 'tool', 'bot', 'game', 'ui', 'ux',
    #     'frontend', 'backend', 'fullstack', 'devops', 'microservice'
    # ]
    
    # 1. ê¸°ì¡´ ì¼ë°˜ í‚¤ì›Œë“œ íŠ¹ì„± ìƒì„±
    print("   ì¼ë°˜ ê¸°ìˆ  í‚¤ì›Œë“œ ì²˜ë¦¬ ì¤‘...")
    for keyword in general_tech_keywords:
        df[f'has_{keyword}'] = df['description'].str.contains(keyword, case=False, na=False).astype(int)
    
    # 2. ìŠ¤íƒë³„ íŠ¹í™” í‚¤ì›Œë“œ íŠ¹ì„± ìƒì„±
    print("   ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ ìŠ¤íƒë³„ íŠ¹í™” í‚¤ì›Œë“œ ì²˜ë¦¬ ì¤‘...")
    for stack_name, keywords in stack_keywords.items():
        print(f"     {stack_name}: {len(keywords)}ê°œ í‚¤ì›Œë“œ")
        
        # ê° í‚¤ì›Œë“œë³„ë¡œ ê°œë³„ íŠ¹ì„± ìƒì„±
        for keyword in keywords:
            df[f'{stack_name}_{keyword}'] = df['description'].str.contains(keyword, case=False, na=False).astype(int)
        
        # ìŠ¤íƒë³„ ì´ í‚¤ì›Œë“œ ê°œìˆ˜ (í•´ë‹¹ ìŠ¤íƒê³¼ ê´€ë ¨ëœ í‚¤ì›Œë“œê°€ ëª‡ ê°œë‚˜ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€)
        stack_columns = [f'{stack_name}_{keyword}' for keyword in keywords]
        df[f'{stack_name}_keyword_count'] = df[stack_columns].sum(axis=1)
        
        # ìŠ¤íƒë³„ í‚¤ì›Œë“œ ë¹„ìœ¨ (ì „ì²´ ë‹¨ì–´ ëŒ€ë¹„ í•´ë‹¹ ìŠ¤íƒ í‚¤ì›Œë“œ ë¹„ìœ¨)
        df[f'{stack_name}_keyword_ratio'] = df[f'{stack_name}_keyword_count'] / (df['description_word_count'] + 1)
        
        # ìŠ¤íƒë³„ í‚¤ì›Œë“œ ì¡´ì¬ ì—¬ë¶€ (í•˜ë‚˜ë¼ë„ ìˆìœ¼ë©´ 1)
        df[f'has_{stack_name}_keywords'] = (df[f'{stack_name}_keyword_count'] > 0).astype(int)
    
    # 3. êµì°¨ ìŠ¤íƒ íŠ¹ì„± (ë³µí•© ìŠ¤íƒ ê°œë°œì ì‹ë³„)
    print("   êµì°¨ ìŠ¤íƒ íŠ¹ì„± ìƒì„± ì¤‘...")
    
    # Full-stack ê´€ë ¨ íŠ¹ì„±
    df['is_fullstack_likely'] = (
        (df['has_frontend_keywords'] == 1) & 
        (df['has_server_keywords'] == 1)
    ).astype(int)
    
    # Mobile ê°œë°œì (Android + iOS)
    df['is_mobile_dev'] = (
        (df['has_android_keywords'] == 1) | 
        (df['has_ios_keywords'] == 1)
    ).astype(int)
    
    # Data-focused ê°œë°œì (ML + Visualization)
    df['is_data_focused'] = (
        (df['has_ml_data_keywords'] == 1) | 
        (df['has_visualization_keywords'] == 1)
    ).astype(int)
    
    # Backend-heavy ê°œë°œì (Server + System)
    df['is_backend_heavy'] = (
        (df['has_server_keywords'] == 1) & 
        (df['has_system_keywords'] == 1)
    ).astype(int)
    
    # 4. í‚¤ì›Œë“œ ë‹¤ì–‘ì„± ì§€ìˆ˜ (ì–¼ë§ˆë‚˜ ë‹¤ì–‘í•œ ìŠ¤íƒì— ê´€ì‹¬ì´ ìˆëŠ”ì§€)
    stack_interest_cols = [f'has_{stack}_keywords' for stack in stack_keywords.keys()]
    df['stack_diversity'] = df[stack_interest_cols].sum(axis=1)
    
    # 5. ì£¼ë ¥ ìŠ¤íƒ ì¶”ì • (í‚¤ì›Œë“œ ê¸°ë°˜)
    stack_count_cols = [f'{stack}_keyword_count' for stack in stack_keywords.keys()]
    
    # ê°€ì¥ ë§ì€ í‚¤ì›Œë“œë¥¼ ê°€ì§„ ìŠ¤íƒì„ ì£¼ë ¥ ìŠ¤íƒìœ¼ë¡œ ì¶”ì •
    stack_counts_array = df[stack_count_cols].values
    df['estimated_main_stack_idx'] = np.argmax(stack_counts_array, axis=1)
    
    # ì£¼ë ¥ ìŠ¤íƒ ì´ë¦„
    stack_names = list(stack_keywords.keys())
    df['estimated_main_stack'] = df['estimated_main_stack_idx'].apply(lambda x: stack_names[x])
    
    # ì£¼ë ¥ ìŠ¤íƒ ì‹ ë¢°ë„ (ì£¼ë ¥ ìŠ¤íƒ í‚¤ì›Œë“œ ìˆ˜ / ì „ì²´ ê¸°ìˆ  í‚¤ì›Œë“œ ìˆ˜)
    df['main_stack_confidence'] = np.max(stack_counts_array, axis=1) / (np.sum(stack_counts_array, axis=1) + 1)
    
    # 6. í†µê³„ ì •ë³´ ì¶œë ¥
    print(f"\nğŸ“Š ìƒì„±ëœ í‚¤ì›Œë“œ íŠ¹ì„± í†µê³„:")
    print(f"   ì¼ë°˜ í‚¤ì›Œë“œ íŠ¹ì„±: {len(general_tech_keywords)}ê°œ")
    
    total_stack_features = 0
    for stack_name, keywords in stack_keywords.items():
        stack_features = len(keywords) + 3  # ê°œë³„ í‚¤ì›Œë“œ + count + ratio + has_keywords
        total_stack_features += stack_features
        
        # ê° ìŠ¤íƒë³„ í‚¤ì›Œë“œ ë§¤ì¹­ í†µê³„
        keyword_count_col = f'{stack_name}_keyword_count'
        has_keywords_col = f'has_{stack_name}_keywords'
        
        avg_keywords = df[keyword_count_col].mean()
        users_with_keywords = df[has_keywords_col].sum()
        percentage = (users_with_keywords / len(df)) * 100
        
        print(f"   {stack_name}: {len(keywords)}ê°œ í‚¤ì›Œë“œ, í‰ê·  {avg_keywords:.1f}ê°œ ë§¤ì¹­, {users_with_keywords}ëª… ({percentage:.1f}%)")
    
    print(f"   ìŠ¤íƒë³„ íŠ¹ì„± ì´ê³„: {total_stack_features}ê°œ")
    print(f"   êµì°¨ ìŠ¤íƒ íŠ¹ì„±: 4ê°œ")
    print(f"   ë‹¤ì–‘ì„± íŠ¹ì„±: 4ê°œ")
    print(f"   ì´ í‚¤ì›Œë“œ ê´€ë ¨ íŠ¹ì„±: {len(general_tech_keywords) + total_stack_features + 8}ê°œ")
    
    # 7. ì£¼ë ¥ ìŠ¤íƒ ì¶”ì • ê²°ê³¼
    print(f"\nğŸ¯ í‚¤ì›Œë“œ ê¸°ë°˜ ì£¼ë ¥ ìŠ¤íƒ ì¶”ì • ê²°ê³¼:")
    estimated_stack_dist = df['estimated_main_stack'].value_counts()
    for stack, count in estimated_stack_dist.items():
        percentage = (count / len(df)) * 100
        print(f"   {stack}: {count}ëª… ({percentage:.1f}%)")
    
    print("âœ… ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ ìŠ¤íƒë³„ íŠ¹í™” í‚¤ì›Œë“œ íŠ¹ì„± ìƒì„± ì™„ë£Œ")
    return df
def improve_text_features(df: pd.DataFrame) -> pd.DataFrame:
    """
    í…ìŠ¤íŠ¸ íŠ¹ì„±ì„ ê°œì„ í•˜ëŠ” í•¨ìˆ˜
    """
    print("ğŸ“ í…ìŠ¤íŠ¸ íŠ¹ì„± ê°œì„  ì¤‘...")
    
    # 1. í…ìŠ¤íŠ¸ ê¸¸ì´ íŠ¹ì„±
    df['description_length'] = df['description'].str.len()
    df['repo_names_length'] = df['repo_names'].str.len()
    
    # 2. ë‹¨ì–´ ìˆ˜ íŠ¹ì„±
    df['description_word_count'] = df['description'].str.split().str.len()
    df['repo_names_word_count'] = df['repo_names'].str.split().str.len()
    
    # 2. ìŠ¤íƒë³„ íŠ¹í™” í‚¤ì›Œë“œ íŠ¹ì„± ìƒì„± (ìƒˆë¡œ ì¶”ê°€)
    df = create_stack_specific_keywords(df)
    
    # 4. ë¹ˆ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ê°œì„ 
    df['has_description'] = (df['description'] != 'no description available').astype(int)
    df['has_repo_names'] = (df['repo_names'] != 'no repository name').astype(int)
    
    # 5. Repository ê°œìˆ˜ ê´€ë ¨ íŠ¹ì„±
    df['repo_count'] = pd.to_numeric(df['repo_count'], errors='coerce').fillna(0)
    df['avg_repo_name_length'] = df['repo_names_length'] / (df['repo_count'] + 1)
    df['is_prolific_dev'] = (df['repo_count'] > df['repo_count'].quantile(0.75)).astype(int)
    
    print("âœ… í…ìŠ¤íŠ¸ íŠ¹ì„± ê°œì„  ì™„ë£Œ")
    return df

def filter_low_variance_features(X: np.ndarray, threshold: float = 0.0005) -> Tuple[np.ndarray, np.ndarray]:
    """
    ë¶„ì‚°ì´ ë‚®ì€ íŠ¹ì„±ë“¤ì„ ì œê±°í•˜ëŠ” í•¨ìˆ˜
    """
    print(f"ğŸ” ë‚®ì€ ë¶„ì‚° íŠ¹ì„± ì œê±° ì¤‘... (ì„ê³„ê°’: {threshold})")
    
    selector = VarianceThreshold(threshold=threshold)
    X_filtered = selector.fit_transform(X)
    selected_features = selector.get_support()

    variances = selector.variances_
    # ë¶„ì‚° ìƒìœ„ 10ê°œ íŠ¹ì„± ì¶œë ¥
    top_indices = np.argsort(variances)[::-1][:10]
    print("\nğŸ”¥ ë¶„ì‚°ì´ ë†’ì€ Top 10 íŠ¹ì„±:")
    for i, idx in enumerate(top_indices, 1):
        print(f"  {i}. Index {idx} - Variance: {variances[idx]:.6f}")

    removed_count = (~selected_features).sum()
    print(f"âœ… {removed_count}ê°œ íŠ¹ì„± ì œê±°ë¨ ({X.shape[1]} â†’ {X_filtered.shape[1]})")
    
    return X_filtered, selected_features

def analyze_stack_distribution(df: pd.DataFrame) -> None:
    """
    ìŠ¤íƒ ë¶„í¬ë¥¼ ë¶„ì„í•˜ëŠ” í•¨ìˆ˜
    """
    print("ğŸ“Š ìŠ¤íƒ ë¶„í¬ ë¶„ì„ ì¤‘...")
    
    # ì „ì²´ ìŠ¤íƒ ë¶„í¬
    stack_counts = df['stack'].value_counts()
    print(f"\nğŸ“‹ ìŠ¤íƒë³„ ìƒ˜í”Œ ìˆ˜:")
    for stack, count in stack_counts.items():
        percentage = (count / len(df)) * 100
        print(f"   {stack}: {count}ê°œ ({percentage:.1f}%)")
    
    # ê· í˜•ë„ ì²´í¬
    min_samples = stack_counts.min()
    max_samples = stack_counts.max()
    imbalance_ratio = max_samples / min_samples
    
    print(f"\nâš–ï¸ í´ë˜ìŠ¤ ê· í˜• ë¶„ì„:")
    print(f"   ìµœì†Œ ìƒ˜í”Œ: {min_samples}ê°œ")
    print(f"   ìµœëŒ€ ìƒ˜í”Œ: {max_samples}ê°œ") 
    print(f"   ë¶ˆê· í˜• ë¹„ìœ¨: {imbalance_ratio:.2f}")
    
    if imbalance_ratio > 10:
        print("   âš ï¸ ì‹¬ê°í•œ í´ë˜ìŠ¤ ë¶ˆê· í˜• ê°ì§€")
    elif imbalance_ratio > 5:
        print("   ğŸŸ¡ ì¤‘ê°„ ìˆ˜ì¤€ì˜ í´ë˜ìŠ¤ ë¶ˆê· í˜•")
    else:
        print("   âœ… ì–‘í˜¸í•œ í´ë˜ìŠ¤ ê· í˜•")

'''def handle_class_imbalance(X, y, target_stacks, strategy='auto'):
    """
    í´ë˜ìŠ¤ ë¶ˆê· í˜•ì„ í•´ê²°í•˜ëŠ” í•¨ìˆ˜
    """
    print(f"\nâš–ï¸ í´ë˜ìŠ¤ ë¶ˆê· í˜• ì²˜ë¦¬ ì¤‘... (ì „ëµ: {strategy})")
    
    # ë©€í‹°ë¼ë²¨ì„ ë‹¨ì¼ ë¼ë²¨ë¡œ ë³€í™˜
    y_single = np.argmax(y, axis=1)
    
    # ì›ë˜ ë¶„í¬ í™•ì¸
    original_distribution = Counter(y_single)
    print(f"ğŸ“Š ì›ë³¸ í´ë˜ìŠ¤ ë¶„í¬:")
    for class_idx, count in original_distribution.items():
        stack_name = target_stacks[class_idx]
        percentage = (count / len(y_single)) * 100
        print(f"   {stack_name} (idx {class_idx}): {count}ê°œ ({percentage:.1f}%)")
    
    # ë¶ˆê· í˜• ì •ë„ ê³„ì‚°
    min_samples = min(original_distribution.values())
    max_samples = max(original_distribution.values())
    imbalance_ratio = max_samples / min_samples
    
    print(f"\nğŸ“ˆ ë¶ˆê· í˜• ë¶„ì„:")
    print(f"   ìµœì†Œ ìƒ˜í”Œ: {min_samples}ê°œ")
    print(f"   ìµœëŒ€ ìƒ˜í”Œ: {max_samples}ê°œ")
    print(f"   ë¶ˆê· í˜• ë¹„ìœ¨: {imbalance_ratio:.2f}")
    
    # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚°
    classes = np.unique(y_single)
    class_weights = compute_class_weight('balanced', classes=classes, y=y_single)
    class_weight_dict = {}
    
    print(f"\nğŸ¯ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜:")
    for i, class_idx in enumerate(classes):
        stack_name = target_stacks[class_idx]
        weight = class_weights[i]
        class_weight_dict[class_idx] = weight
        print(f"   {stack_name}: {weight:.3f}")
    
    # ì „ëµ ìë™ ì„ íƒ
    if strategy == 'auto':
        if not IMBLEARN_AVAILABLE:
            strategy = 'class_weight'
        elif imbalance_ratio > 10:
            strategy = 'smote'
        elif imbalance_ratio > 5:
            strategy = 'smote'
        else:
            strategy = 'class_weight'
    
    X_resampled, y_resampled = X.copy(), y.copy()
    resampling_applied = False
    
    # SMOTE ì ìš© (ê°€ëŠ¥í•œ ê²½ìš°)
    if strategy == 'smote' and IMBLEARN_AVAILABLE and imbalance_ratio > 2:
        print(f"\nğŸ”„ SMOTE ì ìš© ì¤‘...")
        try:
            # ìµœì†Œ ìƒ˜í”Œ ìˆ˜ í™•ì¸ (SMOTEëŠ” k_neighborsê°€ í•„ìš”)
            min_neighbors = min(5, min_samples - 1)
            if min_neighbors >= 1:
                smote = SMOTE(random_state=42, k_neighbors=min_neighbors)
                X_resampled, y_single_resampled = smote.fit_resample(X, y_single)
                
                # ë©€í‹°ë¼ë²¨ í˜•íƒœë¡œ ë³µì›
                y_resampled = np.zeros((len(y_single_resampled), y.shape[1]))
                for i, label in enumerate(y_single_resampled):
                    y_resampled[i, label] = 1
                
                resampling_applied = True
                print(f"âœ… SMOTE ì™„ë£Œ: {X.shape[0]} â†’ {X_resampled.shape[0]} ìƒ˜í”Œ")
            else:
                print(f"âŒ SMOTE ë¶ˆê°€: ìµœì†Œ ìƒ˜í”Œ ìˆ˜ ë¶€ì¡± (min_samples: {min_samples})")
                strategy = 'class_weight'
                
        except Exception as e:
            print(f"âŒ SMOTE ì‹¤íŒ¨: {e}")
            strategy = 'class_weight'
    
    # ìµœì¢… ë¶„í¬ í™•ì¸
    if resampling_applied:
        y_single_final = np.argmax(y_resampled, axis=1)
        final_distribution = Counter(y_single_final)
        print(f"\nğŸ“Š ë¦¬ìƒ˜í”Œë§ í›„ í´ë˜ìŠ¤ ë¶„í¬:")
        for class_idx, count in final_distribution.items():
            stack_name = target_stacks[class_idx]
            percentage = (count / len(y_single_final)) * 100
            print(f"   {stack_name}: {count}ê°œ ({percentage:.1f}%)")
        
        final_imbalance = max(final_distribution.values()) / min(final_distribution.values())
        print(f"   ê°œì„ ëœ ë¶ˆê· í˜• ë¹„ìœ¨: {final_imbalance:.2f}")
    else:
        print(f"\nğŸ’¡ ë¦¬ìƒ˜í”Œë§ ë¯¸ì ìš© â†’ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì‚¬ìš© ê¶Œì¥")
    
    return X_resampled, y_resampled, class_weight_dict, strategy'''
def handle_class_imbalance_multilabel(X, y, target_stacks, strategy='smote_multilabel'):
    """
    ë©€í‹°ë¼ë²¨ ë°ì´í„°ì˜ í´ë˜ìŠ¤ ë¶ˆê· í˜•ì„ í•´ê²°í•˜ëŠ” ê°œì„ ëœ í•¨ìˆ˜
    """
    print(f"\nâš–ï¸ ë©€í‹°ë¼ë²¨ í´ë˜ìŠ¤ ë¶ˆê· í˜• ì²˜ë¦¬ ì¤‘... (ì „ëµ: {strategy})")
    
    # ê° ë¼ë²¨ë³„ ë¶„í¬ í™•ì¸
    print(f"ğŸ“Š ì›ë³¸ ë¼ë²¨ë³„ ë¶„í¬:")
    original_distribution = {}
    for i, stack in enumerate(target_stacks):
        count = np.sum(y[:, i])
        percentage = (count / len(y)) * 100
        original_distribution[stack] = count
        print(f"   {stack}: {count}ê°œ ({percentage:.1f}%)")
    
    # ë¶ˆê· í˜• ì •ë„ ê³„ì‚°
    min_samples = min(original_distribution.values())
    max_samples = max(original_distribution.values())
    imbalance_ratio = max_samples / min_samples
    
    print(f"\nğŸ“ˆ ë¶ˆê· í˜• ë¶„ì„:")
    print(f"   ìµœì†Œ ìƒ˜í”Œ: {min_samples}ê°œ (iOS)")
    print(f"   ìµœëŒ€ ìƒ˜í”Œ: {max_samples}ê°œ (Server)")
    print(f"   ë¶ˆê· í˜• ë¹„ìœ¨: {imbalance_ratio:.2f}")
    
    # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚° (í•­ìƒ ê³„ì‚°)
    class_weight_dict = {}
    total_samples = len(y)
    
    for i, stack in enumerate(target_stacks):
        positive_samples = np.sum(y[:, i])
        if positive_samples > 0:
            # ê· í˜• ê°€ì¤‘ì¹˜ ê³µì‹: total_samples / (2 * positive_samples)
            weight = total_samples / (2 * positive_samples)
            class_weight_dict[i] = weight
        else:
            class_weight_dict[i] = 1.0
    
    print(f"\nğŸ¯ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜:")
    for i, stack in enumerate(target_stacks):
        weight = class_weight_dict[i]
        print(f"   {stack}: {weight:.3f}")
    
    # SMOTE ì ìš© ì—¬ë¶€ ê²°ì •
    if strategy == 'smote_multilabel' and IMBLEARN_AVAILABLE:
        print(f"\nğŸ”„ ë©€í‹°ë¼ë²¨ SMOTE ì ìš© ì¤‘...")
        
        # ë°©ë²• 1: ê° ë¼ë²¨ë³„ë¡œ ê°œë³„ SMOTE ì ìš© í›„ í•©ì¹˜ê¸°
        try:
            X_resampled_all = []
            y_resampled_all = []
            
            # ê°€ì¥ ë§ì€ í´ë˜ìŠ¤ì˜ ìƒ˜í”Œ ìˆ˜ë¥¼ ëª©í‘œë¡œ ì„¤ì •
            target_sample_count = max_samples
            
            # ê° ë¼ë²¨ë³„ë¡œ SMOTE ì ìš©
            for i, stack in enumerate(target_stacks):
                print(f"   ì²˜ë¦¬ ì¤‘: {stack}")
                
                # í˜„ì¬ ë¼ë²¨ì˜ positive/negative ìƒ˜í”Œ ì¶”ì¶œ
                y_binary = y[:, i]
                positive_mask = y_binary == 1
                negative_mask = y_binary == 0
                
                positive_count = np.sum(positive_mask)
                negative_count = np.sum(negative_mask)
                
                if positive_count < 10:  # ë„ˆë¬´ ì ìœ¼ë©´ ìŠ¤í‚µ
                    print(f"     âš ï¸ {stack}: ìƒ˜í”Œì´ ë„ˆë¬´ ì ì–´ SMOTE ìŠ¤í‚µ ({positive_count}ê°œ)")
                    continue
                
                # í˜„ì¬ ë¼ë²¨ì— ëŒ€í•´ SMOTE ì ìš©
                # k_neighborsë¥¼ positive ìƒ˜í”Œ ìˆ˜ì— ë§ê²Œ ì¡°ì •
                k_neighbors = min(5, positive_count - 1)
                if k_neighbors < 1:
                    k_neighbors = 1
                
                # ëª©í‘œ ìƒ˜í”Œ ìˆ˜ ê³„ì‚° (ìµœëŒ€ í´ë˜ìŠ¤ì˜ 70% ì •ë„ë¡œ ì„¤ì •)
                target_positive = min(int(target_sample_count * 0.7), positive_count * 3)
                
                if target_positive > positive_count:
                    # SMOTE ì ìš©
                    sampling_strategy = {1: target_positive}
                    smote = SMOTE(
                        sampling_strategy=sampling_strategy,
                        random_state=42,
                        k_neighbors=k_neighbors
                    )
                    
                    X_resampled_label, y_resampled_label = smote.fit_resample(X, y_binary)
                    
                    # ì¦ê°•ëœ ìƒ˜í”Œë§Œ ì¶”ì¶œ
                    original_indices = np.arange(len(X))
                    augmented_indices = np.arange(len(X), len(X_resampled_label))
                    
                    if len(augmented_indices) > 0:
                        X_augmented = X_resampled_label[augmented_indices]
                        # ë©€í‹°ë¼ë²¨ í˜•íƒœë¡œ ë³€í™˜ (í˜„ì¬ ë¼ë²¨ë§Œ 1, ë‚˜ë¨¸ì§€ëŠ” 0)
                        y_augmented = np.zeros((len(X_augmented), len(target_stacks)))
                        y_augmented[:, i] = 1
                        
                        X_resampled_all.append(X_augmented)
                        y_resampled_all.append(y_augmented)
                        
                        print(f"     âœ… {stack}: {len(X_augmented)}ê°œ ìƒ˜í”Œ ì¦ê°•")
                else:
                    print(f"     âœ… {stack}: ì¶©ë¶„í•œ ìƒ˜í”Œ, ì¦ê°• ë¶ˆí•„ìš”")
            
            # ì›ë³¸ ë°ì´í„°ì™€ ì¦ê°• ë°ì´í„° í•©ì¹˜ê¸°
            if X_resampled_all:
                X_augmented_combined = np.vstack(X_resampled_all)
                y_augmented_combined = np.vstack(y_resampled_all)
                
                X_final = np.vstack([X, X_augmented_combined])
                y_final = np.vstack([y, y_augmented_combined])
                
                print(f"\nâœ… SMOTE ì ìš© ì™„ë£Œ:")
                print(f"   ì›ë³¸: {X.shape[0]}ê°œ â†’ ìµœì¢…: {X_final.shape[0]}ê°œ")
                print(f"   ì¦ê°•ëœ ìƒ˜í”Œ: {X_augmented_combined.shape[0]}ê°œ")
                
                # ìµœì¢… ë¶„í¬ í™•ì¸
                print(f"\nğŸ“Š SMOTE ì ìš© í›„ ë¼ë²¨ë³„ ë¶„í¬:")
                for i, stack in enumerate(target_stacks):
                    count = np.sum(y_final[:, i])
                    percentage = (count / len(y_final)) * 100
                    improvement = count - original_distribution[stack]
                    print(f"   {stack}: {count}ê°œ ({percentage:.1f}%) [+{improvement}]")
                
                # ê°œì„ ëœ ë¶ˆê· í˜• ë¹„ìœ¨ ê³„ì‚°
                final_counts = [np.sum(y_final[:, i]) for i in range(len(target_stacks))]
                final_imbalance = max(final_counts) / min(final_counts)
                print(f"   ê°œì„ ëœ ë¶ˆê· í˜• ë¹„ìœ¨: {imbalance_ratio:.2f} â†’ {final_imbalance:.2f}")
                
                return X_final, y_final, class_weight_dict, 'smote_multilabel'
            
            else:
                print(f"âŒ SMOTE ì ìš© ì‹¤íŒ¨: ì¦ê°•í•  ìˆ˜ ìˆëŠ” ë¼ë²¨ì´ ì—†ìŒ")
                return X, y, class_weight_dict, 'class_weight'
                
        except Exception as e:
            print(f"âŒ SMOTE ì‹¤íŒ¨: {e}")
            print(f"   í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ë°©ë²•ìœ¼ë¡œ ëŒ€ì²´")
            return X, y, class_weight_dict, 'class_weight'
    
    else:
        print(f"\nğŸ’¡ SMOTE ë¯¸ì ìš© â†’ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ë§Œ ì‚¬ìš©")
        if not IMBLEARN_AVAILABLE:
            print(f"   (imbalanced-learn ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ)")
        return X, y, class_weight_dict, 'class_weight'
'''def create_train_test_split_with_balance(X, y, target_stacks, test_size=0.25, random_state=42):
    """
    ê· í˜•ì„ ê³ ë ¤í•œ train/test ë¶„í• 
    """
    print(f"\nğŸ”„ ê· í˜• ê³ ë ¤ ë°ì´í„° ë¶„í•  ì¤‘... (test_size: {test_size})")
    
    # ë©€í‹°ë¼ë²¨ì„ ë‹¨ì¼ ë¼ë²¨ë¡œ ë³€í™˜
    y_single = np.argmax(y, axis=1)
    
    # ê³„ì¸µí™” ë¶„í• 
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=random_state, 
        stratify=y_single
    )
    
    print(f"âœ… ë¶„í•  ì™„ë£Œ:")
    print(f"   í›ˆë ¨ ì„¸íŠ¸: {X_train.shape[0]}ê°œ")
    print(f"   í…ŒìŠ¤íŠ¸ ì„¸íŠ¸: {X_test.shape[0]}ê°œ")
    
    # ë¶„í•  í›„ ê° ì„¸íŠ¸ì˜ í´ë˜ìŠ¤ ë¶„í¬ í™•ì¸
    y_train_single = np.argmax(y_train, axis=1)
    y_test_single = np.argmax(y_test, axis=1)
    
    train_dist = Counter(y_train_single)
    test_dist = Counter(y_test_single)
    
    print(f"\nğŸ“Š ë¶„í•  í›„ í´ë˜ìŠ¤ ë¶„í¬:")
    for class_idx in sorted(train_dist.keys()):
        stack_name = target_stacks[class_idx]
        train_count = train_dist[class_idx]
        test_count = test_dist.get(class_idx, 0)
        train_pct = (train_count / len(y_train_single)) * 100
        test_pct = (test_count / len(y_test_single)) * 100 if test_count > 0 else 0
        print(f"   {stack_name}: í›ˆë ¨ {train_count}ê°œ ({train_pct:.1f}%), í…ŒìŠ¤íŠ¸ {test_count}ê°œ ({test_pct:.1f}%)")
    
    return X_train, X_test, y_train, y_test'''
def create_train_test_split_with_balance(X, y, target_stacks, test_size=0.25, random_state=42):
    print(f"\nğŸ”„ ë©€í‹°ë¼ë²¨ ë°ì´í„° ë¶„í•  ì¤‘... (test_size: {test_size})")
    
    # ë©€í‹°ë¼ë²¨ì—ì„œëŠ” stratify ì—†ì´ ë‹¨ìˆœ ë¶„í• 
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=random_state
    )
    
    print(f"âœ… ë¶„í•  ì™„ë£Œ:")
    print(f"   í›ˆë ¨ ì„¸íŠ¸: {X_train.shape[0]}ê°œ")
    print(f"   í…ŒìŠ¤íŠ¸ ì„¸íŠ¸: {X_test.shape[0]}ê°œ")
    
    # ê° ì„¸íŠ¸ì˜ í´ë˜ìŠ¤ ë¶„í¬ í™•ì¸
    print(f"\nğŸ“Š ë¶„í•  í›„ ê° ìŠ¤íƒ ë¶„í¬:")
    for i, stack_name in enumerate(target_stacks):
        train_count = np.sum(y_train[:, i])
        test_count = np.sum(y_test[:, i])
        train_pct = (train_count / len(y_train)) * 100
        test_pct = (test_count / len(y_test)) * 100
        print(f"   {stack_name}: í›ˆë ¨ {train_count}ê°œ ({train_pct:.1f}%), í…ŒìŠ¤íŠ¸ {test_count}ê°œ ({test_pct:.1f}%)")
    
    return X_train, X_test, y_train, y_test
def main():
    print("ğŸ“ ë°ì´í„° ë¡œë”© ì¤‘...")
    
    # ë°ì´í„° ë¡œë”© (ì¸ì½”ë”© ì—ëŸ¬ ì²˜ë¦¬)
    try:
        with open(file_path, 'r', encoding='utf-8', errors='replace') as f:
            df = pd.read_csv(f)
    except FileNotFoundError:
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
        return
    
    print(f"âœ… ë°ì´í„° ë¡œë”© ì™„ë£Œ. ì´ {len(df)}ê°œ í–‰")
    print(f"ğŸ“Š ì»¬ëŸ¼: {list(df.columns)}")
    
    # ìŠ¤íƒ ë¶„í¬ ë¶„ì„
    analyze_stack_distribution(df)
    
    # 1. ì–¸ì–´ ë°ì´í„° í†µí•©
    print("\nğŸ”„ ì–¸ì–´ ë°ì´í„° í†µí•© ì¤‘...")
    if 'JavaScript' in df.columns and 'TypeScript' in df.columns:
        df["JS"] = df[['JavaScript', 'TypeScript']].sum(axis=1)
        df.drop(columns=['JavaScript', 'TypeScript'], inplace=True)
        print("âœ… JavaScript + TypeScript â†’ JS í†µí•© ì™„ë£Œ")
    
    if 'C' in df.columns and 'C++' in df.columns:
        df["C/C++"] = df[['C', 'C++']].sum(axis=1)
        #df.drop(columns=['C', 'C++'], inplace=True)
        print("âœ… C + C++ â†’ C/C++ í†µí•© ì™„ë£Œ")
    
    # 2. Repository ì´ë¦„ê³¼ ì„¤ëª… ë¶„ë¦¬
    print("\nğŸ“ Repository í…ìŠ¤íŠ¸ ë¶„ë¦¬ ì¤‘...")
    if 'text' in df.columns:
        df[['repo_names', 'description']] = df['text'].apply(lambda x: pd.Series(split_repos(x)))
        df.drop(columns=['text'], inplace=True)
        print("âœ… Repository ì´ë¦„ê³¼ ì„¤ëª… ë¶„ë¦¬ ì™„ë£Œ")
    
    # 3. Stack ì²˜ë¦¬ (ë©€í‹°ìŠ¤íƒ ì§€ì›)
    if 'stack' in df.columns:
        print("\nğŸ”„ Stack ë°ì´í„° ì²˜ë¦¬ ì¤‘...")
        
        # &ìœ¼ë¡œ ë¶„ë¦¬ëœ ë©€í‹°ìŠ¤íƒì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        df['stack_list'] = df['stack'].apply(process_stack)
        
        # ìŠ¤íƒ í†µê³„ ì •ë³´
        all_stacks = []
        for stack_list in df['stack_list']:
            all_stacks.extend(stack_list)
        
        stack_counts = Counter(all_stacks)
        print(f"ğŸ“Š ì „ì²´ ê³ ìœ  ìŠ¤íƒ ìˆ˜: {len(set(all_stacks))}")
        print(f"ğŸ“Š ìŠ¤íƒë³„ ë¶„í¬:")
        for stack, count in stack_counts.most_common():
            print(f"   {stack}: {count}íšŒ")
        
        print("âœ… Stack ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ")
    df.drop(columns=['C', 'C++'], inplace=True)
    # 4. ì–¸ì–´ ì»¬ëŸ¼ í™•ì¸ ë° íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§
    print("\nğŸ“Š ì–¸ì–´ ë°ì´í„° í™•ì¸ ë° íŠ¹ì„± ìƒì„± ì¤‘...")
    exclude_columns = {'user_ID', 'username', 'repo_count', 'repo_names', 'description', 'stack', 'stack_list', 'note'}
    language_columns = [col for col in df.columns if col not in exclude_columns and df[col].dtype in ['int64', 'float64']]
    
    print(f"ğŸ¯ ì–¸ì–´ ì»¬ëŸ¼: {language_columns}")
    
    # ì–¸ì–´ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì ìš©
    df = create_language_features(df, language_columns)
    
    # 5. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
    print("\nğŸ§¹ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ì¤‘...")
    df['description'] = df['description'].fillna('').apply(clean_text)
    df['repo_names'] = df['repo_names'].fillna('').apply(clean_text)
    df['description'] = df['description'].replace('', 'no description available')
    df['repo_names'] = df['repo_names'].replace('', 'no repository name')
    
    # í…ìŠ¤íŠ¸ íŠ¹ì„± ê°œì„  ì ìš©
    df = improve_text_features(df)
    
    # 6. BERT ì„ë² ë”© ìƒì„±
    print("\nğŸ¤– BERT ëª¨ë¸ ë¡œë”© ì¤‘...")
    
    try:
        model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')
        print("âœ… BERT ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
        
        # ìœ ë‹ˆí¬í•œ í…ìŠ¤íŠ¸ë§Œ ì„ë² ë”©í•˜ì—¬ ì¤‘ë³µ ê³„ì‚° ë°©ì§€
        unique_descriptions = df['description'].unique()
        unique_repo_names = df['repo_names'].unique()
        
        print(f"ğŸ“ ìœ ë‹ˆí¬ Description ì„ë² ë”© ìƒì„± ì¤‘... ({len(unique_descriptions)}ê°œ)")
        unique_desc_embeddings = model.encode(
            unique_descriptions.tolist(),
            show_progress_bar=True,
            batch_size=32,
            convert_to_numpy=True
        )
        
        print(f"ğŸ“ ìœ ë‹ˆí¬ Repository names ì„ë² ë”© ìƒì„± ì¤‘... ({len(unique_repo_names)}ê°œ)")
        unique_name_embeddings = model.encode(
            unique_repo_names.tolist(),
            show_progress_bar=True,
            batch_size=32,
            convert_to_numpy=True
        )
        
        # ì„ë² ë”© ë§¤í•‘
        desc_embedding_dict = dict(zip(unique_descriptions, unique_desc_embeddings))
        name_embedding_dict = dict(zip(unique_repo_names, unique_name_embeddings))
        
        # DataFrameì— ë§¤í•‘
        description_embeddings = np.array([desc_embedding_dict[desc] for desc in df['description']])
        name_embeddings = np.array([name_embedding_dict[name] for name in df['repo_names']])
        
        print("âœ… BERT ì„ë² ë”© ìƒì„± ì™„ë£Œ")
        
    except Exception as e:
        print(f"âŒ BERT ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
        return
    
    # 7. ìµœì¢… íŠ¹ì„± ì¡°í•©
    print("\nğŸ”— ìµœì¢… íŠ¹ì„± ì¡°í•© ì¤‘...")
    
    # ì–¸ì–´ íŠ¹ì„± (ê¸°ì¡´ + ìƒˆë¡œ ìƒì„±ëœ íŠ¹ì„±)
    language_feature_cols = language_columns + [
        'num_languages', 'main_language_ratio', 
        'language_diversity', 'frontend_lang_ratio', 'backend_lang_ratio',
        'mobile_lang_ratio', 'system_lang_ratio'
    ]
    language_feature_cols = [col for col in language_feature_cols if col in df.columns]
    
    # í…ìŠ¤íŠ¸ íŠ¹ì„±
    text_feature_cols = [
        'description_length', 'repo_names_length', 'description_word_count', 
        'repo_names_word_count', 'has_description', 'has_repo_names',
        'avg_repo_name_length', 'is_prolific_dev'
    ]
    tech_keyword_cols = [col for col in df.columns if col.startswith('has_')]
    text_feature_cols.extend(tech_keyword_cols)
    text_feature_cols = [col for col in text_feature_cols if col in df.columns]
    
    # íŠ¹ì„± ì¡°í•©
    X_lang = df[language_feature_cols].values.astype(np.float32)
    X_text_features = df[text_feature_cols].values.astype(np.float32)
    X_desc = description_embeddings.astype(np.float32)
    X_name = name_embeddings.astype(np.float32)
    
    X_total = np.concatenate([X_lang, X_text_features, X_name, X_desc], axis=1)
    
    print(f"ğŸ“Š íŠ¹ì„± ì¡°í•© ê²°ê³¼:")
    print(f"   ì–¸ì–´ íŠ¹ì„±: {X_lang.shape[1]}ê°œ")
    print(f"   í…ìŠ¤íŠ¸ íŠ¹ì„±: {X_text_features.shape[1]}ê°œ")
    print(f"   Repository ì´ë¦„ ì„ë² ë”©: {X_name.shape[1]}ê°œ")
    print(f"   Description ì„ë² ë”©: {X_desc.shape[1]}ê°œ")
    print(f"   ì´ íŠ¹ì„±: {X_total.shape[1]}ê°œ")
    
    # 8. ë‚®ì€ ë¶„ì‚° íŠ¹ì„± ì œê±°
    X_total, selected_features = filter_low_variance_features(X_total, threshold=0.0005)
    
    
    # 9. íƒ€ê²Ÿ ë³€ìˆ˜ ì²˜ë¦¬ (7ê°œ ì£¼ìš” ìŠ¤íƒë§Œ ì‚¬ìš©)
    target_stacks = ['Server', 'System', 'Visualization', 'Frontend', 'Android', 'ML-Data', 'iOS']

    print(f"ğŸ¯ íƒ€ê²Ÿ ìŠ¤íƒ (7ê°œ): {target_stacks}")

    # ê° ì‚¬ìš©ìì˜ ìŠ¤íƒ ë¦¬ìŠ¤íŠ¸ì—ì„œ íƒ€ê²Ÿ ìŠ¤íƒë§Œ í•„í„°ë§
    def filter_target_stacks(stack_list):
        """íƒ€ê²Ÿ ìŠ¤íƒì— í¬í•¨ëœ ìŠ¤íƒë§Œ í•„í„°ë§"""
        if not stack_list:
            return []
        filtered = [stack for stack in stack_list if stack in target_stacks]
        return list(set(filtered))  # ì¤‘ë³µ ì œê±°

    df['filtered_stack_list'] = df['stack_list'].apply(filter_target_stacks)

    # íƒ€ê²Ÿ ìŠ¤íƒì´ ìˆëŠ” ì‚¬ìš©ìë§Œ í•„í„°ë§
    valid_mask = df['filtered_stack_list'].apply(lambda x: len(x) > 0)
    print(f"ğŸ“Š ìœ íš¨í•œ ìƒ˜í”Œ ìˆ˜: {valid_mask.sum()} / {len(df)}")

    # í•„í„°ë§ëœ ë°ì´í„° í™•ì¸
    print(f"\nğŸ“‹ í•„í„°ë§ëœ ìŠ¤íƒ ë¶„í¬:")
    filtered_stacks_all = []
    for stack_list in df.loc[valid_mask, 'filtered_stack_list']:
        filtered_stacks_all.extend(stack_list)

    filtered_stack_counts = Counter(filtered_stacks_all)
    for stack in target_stacks:
        count = filtered_stack_counts.get(stack, 0)
        percentage = (count / valid_mask.sum()) * 100 if valid_mask.sum() > 0 else 0
        print(f"   {stack}: {count}íšŒ ({percentage:.1f}%)")

    X_filtered = X_total[valid_mask.to_numpy()]
    filtered_stack_lists = df.loc[valid_mask, 'filtered_stack_list'].tolist()

    # ë©€í‹°ë¼ë²¨ ì¸ì½”ë”© (7ê°œ íƒ€ê²Ÿ ìŠ¤íƒ ê¸°ì¤€)
    mlb = MultiLabelBinarizer(classes=target_stacks)
    y_filtered = mlb.fit_transform(filtered_stack_lists)

    print(f"âœ… íƒ€ê²Ÿ ë³€ìˆ˜ ì²˜ë¦¬ ì™„ë£Œ")
    print(f"   â€¢ ìµœì¢… ìƒ˜í”Œ ìˆ˜: {X_filtered.shape[0]}")
    print(f"   â€¢ ë©€í‹°ë¼ë²¨ shape: {y_filtered.shape}")  # (n_samples, 7)ì´ì–´ì•¼ í•¨

    # ê° ì‚¬ìš©ìê°€ ê°€ì§„ ìŠ¤íƒ ê°œìˆ˜ í™•ì¸
    stack_counts_per_user = [len(stack_list) for stack_list in filtered_stack_lists]
    unique_counts = Counter(stack_counts_per_user)
    print(f"   â€¢ ì‚¬ìš©ìë³„ ìŠ¤íƒ ê°œìˆ˜ ë¶„í¬:")
    for count, users in sorted(unique_counts.items()):
        print(f"     {count}ê°œ ìŠ¤íƒ: {users}ëª…")
    
    # â­ 10. í´ë˜ìŠ¤ ë¶ˆê· í˜• ì²˜ë¦¬ (ìƒˆë¡œ ì¶”ê°€ëœ ë¶€ë¶„)
    X_balanced, y_balanced, class_weights, strategy_used = handle_class_imbalance_multilabel(
        X_filtered, y_filtered, target_stacks, strategy='smote_multilabel'
    )
    
    # â­ 11. ê· í˜• ê³ ë ¤ train/test ë¶„í• 
    X_train, X_test, y_train, y_test = create_train_test_split_with_balance(
        X_balanced, y_balanced, target_stacks, test_size=0.25, random_state=42
    )
    
    # 12. ê²°ê³¼ ì €ì¥
    print("\nğŸ’¾ ê²°ê³¼ ì €ì¥ ì¤‘...")
    output_dir = 'C:/Users/jun01/OneDrive/ë°”íƒ• í™”ë©´/ê³ ë ¤ëŒ€/ë°ê³¼/TermProject/pkl_data'
    os.makedirs(output_dir, exist_ok=True)
    
    # ê°œì„ ëœ ë°ì´í„° ì €ì¥ (ì—¬ëŸ¬ ë²„ì „)
    # ë²„ì „ 1: ì›ë³¸ (ê· í˜• ì²˜ë¦¬ ì•ˆëœ ê²ƒ)
    np.save(os.path.join(output_dir, "X_filtered_original.npy"), X_filtered)
    np.save(os.path.join(output_dir, "y_filtered_original.npy"), y_filtered)
    
    # ë²„ì „ 2: ê· í˜• ì²˜ë¦¬ëœ ì „ì²´ ë°ì´í„°
    np.save(os.path.join(output_dir, "X_filtered_balanced.npy"), X_balanced)
    np.save(os.path.join(output_dir, "y_filtered_balanced.npy"), y_balanced)
    
    # ë²„ì „ 3: ê· í˜• ì²˜ë¦¬ + ë¶„í• ëœ ë°ì´í„°
    np.save(os.path.join(output_dir, "X_train_balanced.npy"), X_train)
    np.save(os.path.join(output_dir, "X_test_balanced.npy"), X_test)
    np.save(os.path.join(output_dir, "y_train_balanced.npy"), y_train)
    np.save(os.path.join(output_dir, "y_test_balanced.npy"), y_test)
    
    # ë©”íƒ€ë°ì´í„° ì €ì¥ (í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ í¬í•¨)
    metadata = {
        'target_stacks': target_stacks,
        'language_features': language_feature_cols,
        'text_features': text_feature_cols,
        'selected_features': selected_features,
        'total_features': X_balanced.shape[1],
        'samples': {
            'original': X_filtered.shape[0],
            'balanced': X_balanced.shape[0],
            'train': X_train.shape[0],
            'test': X_test.shape[0]
        },
        'embedding_dims': {
            'description': description_embeddings.shape[1],
            'repo_names': name_embeddings.shape[1]
        },
        'class_weights': class_weights,
        'strategy_used': strategy_used,
        'imbalance_handling': {
            'applied': strategy_used != 'class_weight',
            'method': strategy_used
        }
    }
    
    with open(os.path.join(output_dir, "metadata_enhanced.pkl"), 'wb') as f:
        pickle.dump(metadata, f)
    
    # ì²˜ë¦¬ëœ DataFrameë„ ì €ì¥
    df_filtered = df[valid_mask].copy()
    df_filtered.to_pickle(os.path.join(output_dir, "processed_df_enhanced.pkl"))
    
    # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ë¥¼ ë³„ë„ íŒŒì¼ë¡œ ì €ì¥ (ëª¨ë¸ í•™ìŠµì‹œ ì‰½ê²Œ ë¶ˆëŸ¬ì˜¤ê¸° ìœ„í•´)
    class_weight_for_keras = {}
    for i, stack in enumerate(target_stacks):
        class_weight_for_keras[i] = class_weights.get(i, 1.0)
    
    with open(os.path.join(output_dir, "class_weights.pkl"), 'wb') as f:
        pickle.dump(class_weight_for_keras, f)
    
    print("âœ… ê°œì„ ëœ ë°ì´í„° ì €ì¥ ì™„ë£Œ!")
    print(f"ğŸ“Š ìµœì¢… ê²°ê³¼:")
    print(f"   â€¢ ì›ë³¸ ë°ì´í„°: X={X_filtered.shape}, y={y_filtered.shape}")
    print(f"   â€¢ ê· í˜• ì²˜ë¦¬ëœ ë°ì´í„°: X={X_balanced.shape}, y={y_balanced.shape}")
    print(f"   â€¢ í›ˆë ¨ ë°ì´í„°: X={X_train.shape}, y={y_train.shape}")
    print(f"   â€¢ í…ŒìŠ¤íŠ¸ ë°ì´í„°: X={X_test.shape}, y={y_test.shape}")
    print(f"   â€¢ Target stacks: {target_stacks}")
    print(f"   â€¢ ì–¸ì–´ íŠ¹ì„±: {len(language_feature_cols)}ê°œ")
    print(f"   â€¢ í…ìŠ¤íŠ¸ íŠ¹ì„±: {len(text_feature_cols)}ê°œ")
    print(f"   â€¢ BERT ì„ë² ë”©: {description_embeddings.shape[1] + name_embeddings.shape[1]}ì°¨ì›")
    print(f"   â€¢ ì‚¬ìš©ëœ ê· í˜• ì „ëµ: {strategy_used}")
    
    # ìµœì¢… ìŠ¤íƒë³„ ë¶„í¬ ì¬í™•ì¸
    print(f"\nğŸ“Š ìµœì¢… ìŠ¤íƒë³„ ìƒ˜í”Œ ë¶„í¬ (ê· í˜• ì²˜ë¦¬ í›„):")
    y_balanced_single = np.argmax(y_balanced, axis=1)
    final_distribution = Counter(y_balanced_single)
    for class_idx, count in final_distribution.items():
        stack_name = target_stacks[class_idx]
        percentage = (count / len(y_balanced_single)) * 100
        print(f"   {stack_name}: {count}ê°œ ({percentage:.1f}%)")
    
    '''# 13. ì‚¬ìš© ê°€ì´ë“œ ì¶œë ¥
    print(f"\nğŸ“– ì‚¬ìš© ê°€ì´ë“œ:")
    print(f"=" * 50)
    print(f"ğŸ”¹ ì›ë³¸ ë°ì´í„° ì‚¬ìš©:")
    print(f"   X = np.load('X_filtered_original.npy')")
    print(f"   y = np.load('y_filtered_original.npy')")
    print(f"")
    print(f"ğŸ”¹ ê· í˜• ì²˜ë¦¬ëœ ë°ì´í„° ì‚¬ìš©:")
    print(f"   X = np.load('X_filtered_balanced.npy')")
    print(f"   y = np.load('y_filtered_balanced.npy')")
    print(f"")
    print(f"ğŸ”¹ ë°”ë¡œ í•™ìŠµ ê°€ëŠ¥í•œ ë¶„í•  ë°ì´í„°:")
    print(f"   X_train = np.load('X_train_balanced.npy')")
    print(f"   X_test = np.load('X_test_balanced.npy')")
    print(f"   y_train = np.load('y_train_balanced.npy')")
    print(f"   y_test = np.load('y_test_balanced.npy')")
    print(f"")
    print(f"ğŸ”¹ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì‚¬ìš©:")
    print(f"   import pickle")
    print(f"   with open('class_weights.pkl', 'rb') as f:")
    print(f"       class_weights = pickle.load(f)")
    print(f"   # ëª¨ë¸ í•™ìŠµì‹œ:")
    print(f"   model.fit(X_train, y_train, class_weight=class_weights, ...)")
    print(f"=" * 50)'''
    
    # 14. ì˜ˆìƒ ì„±ëŠ¥ í–¥ìƒ ìš”ì•½
    original_imbalance = max(Counter(np.argmax(y_filtered, axis=1)).values()) / min(Counter(np.argmax(y_filtered, axis=1)).values())
    final_imbalance = max(final_distribution.values()) / min(final_distribution.values()) if len(final_distribution) > 1 else 1.0
    
    print(f"\nğŸ¯ ì˜ˆìƒ ì„±ëŠ¥ í–¥ìƒ:")
    print(f"   â€¢ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§: +3-5%p")
    print(f"   â€¢ í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ê²°: +2-4%p")
    print(f"     - ì›ë³¸ ë¶ˆê· í˜• ë¹„ìœ¨: {original_imbalance:.2f}")
    print(f"     - ì²˜ë¦¬ í›„ ë¶ˆê· í˜• ë¹„ìœ¨: {final_imbalance:.2f}")
    print(f"   â€¢ í…ìŠ¤íŠ¸ í’ˆì§ˆ ê°œì„ : +1-2%p")
    print(f"   â€¢ ì´ ì˜ˆìƒ í–¥ìƒ: +6-11%p (66.7% â†’ 73-78%)")

if __name__ == "__main__":
    main()