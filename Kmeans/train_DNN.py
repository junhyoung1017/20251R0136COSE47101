import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.feature_selection import SelectKBest, f_classif
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import warnings
warnings.filterwarnings('ignore')

# TensorFlow/Keras ì„í¬íŠ¸
try:
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import layers
    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
    from tensorflow.keras.utils import to_categorical
    print(f"âœ… TensorFlow {tf.__version__} ë¡œë“œ ì™„ë£Œ")
    TENSORFLOW_AVAILABLE = True
except ImportError:
    print("âŒ TensorFlowë¥¼ ì„¤ì¹˜í•´ì£¼ì„¸ìš”: pip install tensorflow")
    TENSORFLOW_AVAILABLE = False

# í•œê¸€ í°íŠ¸ ì„¤ì • (matplotlib)
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False

def load_and_explore_data(pkl_path):
    """
    í”¼í´ íŒŒì¼ì„ ë¡œë“œí•˜ê³  ë°ì´í„°ë¥¼ íƒìƒ‰í•˜ëŠ” í•¨ìˆ˜
    """
    print("ğŸ“‚ ë°ì´í„° ë¡œë”© ì¤‘...")
    df = pd.read_pickle(pkl_path)
    
    print(f"âœ… ë°ì´í„° ë¡œë”© ì™„ë£Œ!")
    print(f"ğŸ“Š ë°ì´í„° í¬ê¸°: {df.shape}")
    print(f"ğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")
    
    # ì»¬ëŸ¼ ë¶„ë¥˜
    basic_cols = [col for col in df.columns if not col.startswith('bert_')]
    bert_name_cols = [col for col in df.columns if col.startswith('bert_name_')]
    bert_desc_cols = [col for col in df.columns if col.startswith('bert_desc_')]
    
    print(f"\nğŸ“‹ ì»¬ëŸ¼ êµ¬ì„±:")
    print(f"â€¢ ê¸°ë³¸ ì»¬ëŸ¼: {len(basic_cols)}ê°œ")
    print(f"â€¢ BERT name ì„ë² ë”©: {len(bert_name_cols)}ê°œ")
    print(f"â€¢ BERT description ì„ë² ë”©: {len(bert_desc_cols)}ê°œ")
    
    # íƒ€ê²Ÿ ë³€ìˆ˜ í™•ì¸
    if 'stack' in df.columns:
        print(f"\nğŸ¯ íƒ€ê²Ÿ ë³€ìˆ˜ (stack) ë¶„í¬:")
        stack_counts = df['stack'].value_counts()
        print(stack_counts)
        
        # ê²°ì¸¡ì¹˜ í™•ì¸
        null_count = df['stack'].isnull().sum()
        print(f"\nâ— ê²°ì¸¡ì¹˜: {null_count}ê°œ ({null_count/len(df)*100:.2f}%)")
        
        return df, basic_cols, bert_name_cols, bert_desc_cols, stack_counts
    else:
        print("âŒ 'stack' ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
        return df, basic_cols, bert_name_cols, bert_desc_cols, None

def prepare_features(df, basic_cols, bert_name_cols, bert_desc_cols):
    """
    ëª¨ë¸ í•™ìŠµì„ ìœ„í•œ í”¼ì²˜ ì¤€ë¹„
    """
    print("\nğŸ”§ í”¼ì²˜ ì¤€ë¹„ ì¤‘...")
    
    # 1. ì–¸ì–´ í†µê³„ í”¼ì²˜ ì¶”ì¶œ
    exclude_cols = {'user_ID', 'username', 'repo_names', 'description', 'stack', 'note'}
    language_cols = [col for col in basic_cols if col not in exclude_cols and df[col].dtype in ['int64', 'float64']]
    
    print(f"ğŸ¯ ì–¸ì–´ í†µê³„ í”¼ì²˜: {language_cols}")
    
    # 2. ê¸°ë³¸ ì •ë³´ í”¼ì²˜
    basic_info_cols = ['repo_count'] if 'repo_count' in df.columns else []
    
    # 3. ì „ì²´ í”¼ì²˜ ê²°í•©
    feature_columns = language_cols + basic_info_cols + bert_name_cols + bert_desc_cols
    
    print(f"ğŸ“Š ì´ í”¼ì²˜ ìˆ˜: {len(feature_columns)}")
    print(f"â€¢ ì–¸ì–´ í†µê³„: {len(language_cols)}ê°œ")
    print(f"â€¢ ê¸°ë³¸ ì •ë³´: {len(basic_info_cols)}ê°œ") 
    print(f"â€¢ BERT name: {len(bert_name_cols)}ê°œ")
    print(f"â€¢ BERT desc: {len(bert_desc_cols)}ê°œ")
    
    # í”¼ì²˜ ë°ì´í„° ì¶”ì¶œ
    X = df[feature_columns].copy()
    
    # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
    print(f"\nğŸ” ê²°ì¸¡ì¹˜ í™•ì¸:")
    null_counts = X.isnull().sum()
    if null_counts.sum() > 0:
        print(f"ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” ì»¬ëŸ¼: {null_counts[null_counts > 0]}")
        X = X.fillna(0)  # ê²°ì¸¡ì¹˜ë¥¼ 0ìœ¼ë¡œ ì±„ì›€
        print("âœ… ê²°ì¸¡ì¹˜ë¥¼ 0ìœ¼ë¡œ ì²˜ë¦¬í–ˆìŠµë‹ˆë‹¤.")
    else:
        print("âœ… ê²°ì¸¡ì¹˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    return X, feature_columns, language_cols

def prepare_target(df):
    """
    íƒ€ê²Ÿ ë³€ìˆ˜ ì¤€ë¹„
    """
    print("\nğŸ¯ íƒ€ê²Ÿ ë³€ìˆ˜ ì¤€ë¹„ ì¤‘...")
    
    # ê²°ì¸¡ì¹˜ ì œê±°
    valid_mask = df['stack'].notna()
    print(f"ìœ íš¨í•œ íƒ€ê²Ÿ ë°ì´í„°: {valid_mask.sum()}ê°œ")
    
    if valid_mask.sum() == 0:
        raise ValueError("ëª¨ë“  íƒ€ê²Ÿ ê°’ì´ ê²°ì¸¡ì¹˜ì…ë‹ˆë‹¤!")
    
    # ë ˆì´ë¸” ì¸ì½”ë”©
    le = LabelEncoder()
    y_encoded = le.fit_transform(df.loc[valid_mask, 'stack'])
    
    print(f"âœ… íƒ€ê²Ÿ í´ë˜ìŠ¤ ìˆ˜: {len(le.classes_)}")
    print(f"í´ë˜ìŠ¤ ëª©ë¡: {list(le.classes_)}")
    
    return y_encoded, valid_mask, le

def create_neural_network(input_dim, num_classes, model_type='basic'):
    """
    ë”¥ëŸ¬ë‹ ëª¨ë¸ ìƒì„±
    """
    if model_type == 'basic':
        # ê¸°ë³¸ í”¼ë“œí¬ì›Œë“œ ë„¤íŠ¸ì›Œí¬
        model = keras.Sequential([
            layers.Dense(512, activation='relu', input_shape=(input_dim,)),
            layers.BatchNormalization(),
            layers.Dropout(0.3),
            
            layers.Dense(256, activation='relu'),
            layers.BatchNormalization(),
            layers.Dropout(0.3),
            
            layers.Dense(128, activation='relu'),
            layers.BatchNormalization(),
            layers.Dropout(0.2),
            
            layers.Dense(64, activation='relu'),
            layers.Dropout(0.2),
            
            layers.Dense(num_classes, activation='softmax')
        ])
        
    elif model_type == 'deep':
        # ë” ê¹Šì€ ë„¤íŠ¸ì›Œí¬
        model = keras.Sequential([
            layers.Dense(1024, activation='relu', input_shape=(input_dim,)),
            layers.BatchNormalization(),
            layers.Dropout(0.4),
            
            layers.Dense(512, activation='relu'),
            layers.BatchNormalization(),
            layers.Dropout(0.3),
            
            layers.Dense(256, activation='relu'),
            layers.BatchNormalization(),
            layers.Dropout(0.3),
            
            layers.Dense(128, activation='relu'),
            layers.BatchNormalization(),
            layers.Dropout(0.2),
            
            layers.Dense(64, activation='relu'),
            layers.Dropout(0.2),
            
            layers.Dense(32, activation='relu'),
            layers.Dropout(0.1),
            
            layers.Dense(num_classes, activation='softmax')
        ])
        
    elif model_type == 'multi_input':
        # ë‹¤ì¤‘ ì…ë ¥ ëª¨ë¸ (ì–¸ì–´í†µê³„ + BERT ì„ë² ë”© ë¶„ë¦¬)
        # ì–¸ì–´ í†µê³„ ì…ë ¥
        lang_input = keras.Input(shape=(20,), name='language_stats')  # ì–¸ì–´ í”¼ì²˜ ìˆ˜ì— ë§ê²Œ ì¡°ì •
        lang_dense = layers.Dense(64, activation='relu')(lang_input)
        lang_dense = layers.Dropout(0.2)(lang_dense)
        
        # BERT ì„ë² ë”© ì…ë ¥
        bert_input = keras.Input(shape=(768,), name='bert_embeddings')  # BERT ì„ë² ë”© ì°¨ì›
        bert_dense = layers.Dense(256, activation='relu')(bert_input)
        bert_dense = layers.BatchNormalization()(bert_dense)
        bert_dense = layers.Dropout(0.3)(bert_dense)
        bert_dense = layers.Dense(128, activation='relu')(bert_dense)
        bert_dense = layers.Dropout(0.2)(bert_dense)
        
        # ë‘ ì…ë ¥ ê²°í•©
        combined = layers.concatenate([lang_dense, bert_dense])
        combined = layers.Dense(128, activation='relu')(combined)
        combined = layers.BatchNormalization()(combined)
        combined = layers.Dropout(0.3)(combined)
        combined = layers.Dense(64, activation='relu')(combined)
        combined = layers.Dropout(0.2)(combined)
        
        output = layers.Dense(num_classes, activation='softmax')(combined)
        
        model = keras.Model(inputs=[lang_input, bert_input], outputs=output)
    
    return model

def train_neural_networks(X_train, y_train, X_val, y_val, num_classes):
    """
    ì—¬ëŸ¬ ë”¥ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ
    """
    print("\nğŸ§  ë”¥ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ ì¤‘...")
    
    # íƒ€ê²Ÿì„ ì›-í•« ì¸ì½”ë”©
    y_train_cat = to_categorical(y_train, num_classes)
    y_val_cat = to_categorical(y_val, num_classes)
    
    # ì½œë°± ì •ì˜
    early_stopping = EarlyStopping(
        monitor='val_accuracy',
        patience=15,
        restore_best_weights=True,
        verbose=1
    )
    
    reduce_lr = ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,
        patience=7,
        min_lr=1e-7,
        verbose=1
    )
    
    callbacks = [early_stopping, reduce_lr]
    
    dl_results = {}
    
    # 1. ê¸°ë³¸ ì‹ ê²½ë§
    print("\nğŸ”„ ê¸°ë³¸ ì‹ ê²½ë§ í•™ìŠµ ì¤‘...")
    basic_model = create_neural_network(X_train.shape[1], num_classes, 'basic')
    basic_model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=0.001),
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )
    
    history_basic = basic_model.fit(
        X_train, y_train_cat,
        validation_data=(X_val, y_val_cat),
        epochs=100,
        batch_size=32,
        callbacks=callbacks,
        verbose=0
    )
    
    # í‰ê°€
    val_loss, val_acc = basic_model.evaluate(X_val, y_val_cat, verbose=0)
    dl_results['Neural Network (Basic)'] = {
        'model': basic_model,
        'accuracy': val_acc,
        'history': history_basic
    }
    print(f"âœ… ê¸°ë³¸ ì‹ ê²½ë§ ì™„ë£Œ! ê²€ì¦ ì •í™•ë„: {val_acc:.4f}")
    
    # 2. ê¹Šì€ ì‹ ê²½ë§
    print("\nğŸ”„ ê¹Šì€ ì‹ ê²½ë§ í•™ìŠµ ì¤‘...")
    deep_model = create_neural_network(X_train.shape[1], num_classes, 'deep')
    deep_model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=0.001),
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )
    
    history_deep = deep_model.fit(
        X_train, y_train_cat,
        validation_data=(X_val, y_val_cat),
        epochs=100,
        batch_size=32,
        callbacks=callbacks,
        verbose=0
    )
    
    val_loss, val_acc = deep_model.evaluate(X_val, y_val_cat, verbose=0)
    dl_results['Neural Network (Deep)'] = {
        'model': deep_model,
        'accuracy': val_acc,
        'history': history_deep
    }
    print(f"âœ… ê¹Šì€ ì‹ ê²½ë§ ì™„ë£Œ! ê²€ì¦ ì •í™•ë„: {val_acc:.4f}")
    
    return dl_results

'''def train_traditional_models(X_train, y_train, X_test, y_test):
    """
    ì „í†µì ì¸ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ
    """
    print("\nğŸ¤– ì „í†µì ì¸ ML ëª¨ë¸ í•™ìŠµ ì¤‘...")
    
    models = {
        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),
        'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),
        'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),
        'SVM': SVC(random_state=42, probability=True)
    }
    
    results = {}
    
    for name, model in models.items():
        print(f"\nğŸ”„ {name} í•™ìŠµ ì¤‘...")
        
        # ëª¨ë¸ í•™ìŠµ
        model.fit(X_train, y_train)
        
        # ì˜ˆì¸¡
        y_pred = model.predict(X_test)
        
        # í‰ê°€
        accuracy = accuracy_score(y_test, y_pred)
        
        # êµì°¨ ê²€ì¦
        cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')
        
        results[name] = {
            'model': model,
            'accuracy': accuracy,
            'cv_mean': cv_scores.mean(),
            'cv_std': cv_scores.std(),
            'predictions': y_pred
        }
        
        print(f"âœ… {name} ì™„ë£Œ!")
        print(f"   í…ŒìŠ¤íŠ¸ ì •í™•ë„: {accuracy:.4f}")
        print(f"   êµì°¨ ê²€ì¦ ì •í™•ë„: {cv_scores.mean():.4f} (Â±{cv_scores.std():.4f})")
    
    return results'''

def evaluate_all_models(ml_results, dl_results, X_test, y_test, label_encoder):
    """
    ëª¨ë“  ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ë° í‰ê°€
    """
    print("\nğŸ† ì „ì²´ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ:")
    print("-" * 70)
    
    all_results = {}
    
    '''# ì „í†µì ì¸ ML ëª¨ë¸ ê²°ê³¼
    for name, result in ml_results.items():
        all_results[name] = result['accuracy']
        print(f"{name:25} | í…ŒìŠ¤íŠ¸: {result['accuracy']:.4f} | CV: {result['cv_mean']:.4f}Â±{result['cv_std']:.4f}")'''
    
    # ë”¥ëŸ¬ë‹ ëª¨ë¸ ê²°ê³¼
    if TENSORFLOW_AVAILABLE and dl_results:
        for name, result in dl_results.items():
            all_results[name] = result['accuracy']
            print(f"{name:25} | ê²€ì¦: {result['accuracy']:.4f}")
    
    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì„ íƒ
    best_model_name = max(all_results.keys(), key=lambda x: all_results[x])
    best_accuracy = all_results[best_model_name]
    
    print(f"\nğŸ¥‡ ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model_name}")
    print(f"   ì •í™•ë„: {best_accuracy:.4f}")
    
    # ìµœê³  ëª¨ë¸ì˜ ìƒì„¸ í‰ê°€
    if best_model_name in ml_results:
        # ì „í†µì ì¸ ML ëª¨ë¸
        best_result = ml_results[best_model_name]
        y_pred = best_result['predictions']
        
        # í´ë˜ìŠ¤ ì´ë¦„ìœ¼ë¡œ ë³€í™˜
        y_test_labels = label_encoder.inverse_transform(y_test)
        y_pred_labels = label_encoder.inverse_transform(y_pred)
        
        print(f"\nğŸ“Š {best_model_name} ìƒì„¸ í‰ê°€:")
        print("\në¶„ë¥˜ ë³´ê³ ì„œ:")
        print(classification_report(y_test_labels, y_pred_labels))
        
        return ml_results[best_model_name]['model'], best_model_name
    
    else:
        # ë”¥ëŸ¬ë‹ ëª¨ë¸
        best_model = dl_results[best_model_name]['model']
        
        # ë”¥ëŸ¬ë‹ ëª¨ë¸ ì˜ˆì¸¡
        y_pred_proba = best_model.predict(X_test, verbose=0)
        y_pred = np.argmax(y_pred_proba, axis=1)
        
        # í´ë˜ìŠ¤ ì´ë¦„ìœ¼ë¡œ ë³€í™˜
        y_test_labels = label_encoder.inverse_transform(y_test)
        y_pred_labels = label_encoder.inverse_transform(y_pred)
        
        print(f"\nğŸ“Š {best_model_name} ìƒì„¸ í‰ê°€:")
        print("\në¶„ë¥˜ ë³´ê³ ì„œ:")
        print(classification_report(y_test_labels, y_pred_labels))
        
        return best_model, best_model_name

def visualize_results(ml_results, dl_results, y_test, label_encoder):
    """
    ê²°ê³¼ ì‹œê°í™”
    """
    print("\nğŸ“ˆ ê²°ê³¼ ì‹œê°í™” ì¤‘...")
    
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    
    # 1. ëª¨ë“  ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ
    all_models = list(ml_results.keys())
    all_accuracies = [ml_results[name]['accuracy'] for name in all_models]
    
    if TENSORFLOW_AVAILABLE and dl_results:
        dl_models = list(dl_results.keys())
        dl_accuracies = [dl_results[name]['accuracy'] for name in dl_models]
        all_models.extend(dl_models)
        all_accuracies.extend(dl_accuracies)
    
    colors = ['skyblue'] * len(ml_results) + ['lightcoral'] * len(dl_results)
    
    bars = axes[0, 0].bar(range(len(all_models)), all_accuracies, color=colors, alpha=0.7)
    axes[0, 0].set_title('All Models Performance Comparison')
    axes[0, 0].set_ylabel('Accuracy')
    axes[0, 0].set_xticks(range(len(all_models)))
    axes[0, 0].set_xticklabels(all_models, rotation=45, ha='right')
    
    # ë²”ë¡€ ì¶”ê°€
    import matplotlib.patches as mpatches
    ml_patch = mpatches.Patch(color='skyblue', label='Traditional ML')
    dl_patch = mpatches.Patch(color='lightcoral', label='Deep Learning')
    axes[0, 0].legend(handles=[ml_patch, dl_patch])
    
    # 2. ë”¥ëŸ¬ë‹ í•™ìŠµ ê³¡ì„  (ì²« ë²ˆì§¸ ëª¨ë¸)
    if TENSORFLOW_AVAILABLE and dl_results:
        first_dl_model = list(dl_results.keys())[0]
        history = dl_results[first_dl_model]['history']
        
        axes[0, 1].plot(history.history['accuracy'], label='Training Accuracy')
        axes[0, 1].plot(history.history['val_accuracy'], label='Validation Accuracy')
        axes[0, 1].set_title(f'{first_dl_model} - Learning Curves')
        axes[0, 1].set_xlabel('Epoch')
        axes[0, 1].set_ylabel('Accuracy')
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)
    else:
        axes[0, 1].text(0.5, 0.5, 'No Deep Learning Results', 
                       ha='center', va='center', transform=axes[0, 1].transAxes)
    
    # 3. ìµœê³  ì„±ëŠ¥ ëª¨ë¸ì˜ í˜¼ë™ í–‰ë ¬
    best_model_name = max(all_models, key=lambda x: 
                         ml_results[x]['accuracy'] if x in ml_results 
                         else dl_results[x]['accuracy'])
    
    if best_model_name in ml_results:
        y_pred = ml_results[best_model_name]['predictions']
    else:
        # ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’ ê³„ì‚° í•„ìš” (ì—¬ê¸°ì„œëŠ” ì„ì‹œë¡œ ì²˜ë¦¬)
        y_pred = y_test  # ì‹¤ì œë¡œëŠ” ëª¨ë¸ ì˜ˆì¸¡ê°’ì„ ì‚¬ìš©í•´ì•¼ í•¨
    
    # í´ë˜ìŠ¤ ì´ë¦„ìœ¼ë¡œ ë³€í™˜
    y_test_labels = label_encoder.inverse_transform(y_test)
    if best_model_name in ml_results:
        y_pred_labels = label_encoder.inverse_transform(y_pred)
    else:
        y_pred_labels = y_test_labels  # ì„ì‹œ ì²˜ë¦¬
    
    cm = confusion_matrix(y_test_labels, y_pred_labels)
    
    # 3,4ë²ˆ subplotì„ í•©ì³ì„œ í˜¼ë™ í–‰ë ¬ ê·¸ë¦¬ê¸°
    axes[1, 0].remove()
    axes[1, 1].remove()
    ax_cm = plt.subplot(2, 2, (3, 4))
    
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=label_encoder.classes_, 
                yticklabels=label_encoder.classes_,
                ax=ax_cm)
    ax_cm.set_title(f'{best_model_name} - Confusion Matrix')
    ax_cm.set_ylabel('True Label')
    ax_cm.set_xlabel('Predicted Label')
    
    plt.tight_layout()
    plt.show()

def main():
    # íŒŒì¼ ê²½ë¡œ ì„¤ì •
    pkl_path = 'C:/Users/jun01/OneDrive/ë°”íƒ• í™”ë©´/ê³ ë ¤ëŒ€/ë°ê³¼/TermProject/github_profiles_with_bert_processed.pkl'
    
    try:
        # 1. ë°ì´í„° ë¡œë“œ ë° íƒìƒ‰
        df, basic_cols, bert_name_cols, bert_desc_cols, stack_counts = load_and_explore_data(pkl_path)
        
        if stack_counts is None:
            return
        
        # 2. í”¼ì²˜ ì¤€ë¹„
        X, feature_columns, language_cols = prepare_features(df, basic_cols, bert_name_cols, bert_desc_cols)
        
        # 3. íƒ€ê²Ÿ ì¤€ë¹„
        y, valid_mask, label_encoder = prepare_target(df)
        
        # ìœ íš¨í•œ ë°ì´í„°ë§Œ ì„ íƒ
        X = X.loc[valid_mask].reset_index(drop=True)
        
        print(f"\nğŸ“Š ìµœì¢… ë°ì´í„° í¬ê¸°: {X.shape}")
        print(f"íƒ€ê²Ÿ ë°ì´í„° í¬ê¸°: {len(y)}")
        
        # 4. ë°ì´í„° ë¶„í• 
        print("\nğŸ”€ ë°ì´í„° ë¶„í•  ì¤‘...")
        X_temp, X_test, y_temp, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        X_train, X_val, y_train, y_val = train_test_split(
            X_temp, y_temp, test_size=0.2, random_state=42, stratify=y_temp
        )
        
        print(f"í•™ìŠµ ë°ì´í„°: {X_train.shape}")
        print(f"ê²€ì¦ ë°ì´í„°: {X_val.shape}")
        print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {X_test.shape}")
        
        # 5. í”¼ì²˜ ìŠ¤ì¼€ì¼ë§
        print("\nâš–ï¸ í”¼ì²˜ ìŠ¤ì¼€ì¼ë§ ì¤‘...")
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_val_scaled = scaler.transform(X_val)
        X_test_scaled = scaler.transform(X_test)
        
        # 6. ì „í†µì ì¸ ML ëª¨ë¸ í•™ìŠµ
        ml_results = train_traditional_models(X_train_scaled, y_train, X_test_scaled, y_test)
        
        # 7. ë”¥ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ
        dl_results = {}
        if TENSORFLOW_AVAILABLE:
            dl_results = train_neural_networks(X_train_scaled, y_train, X_val_scaled, y_val, len(label_encoder.classes_))
        else:
            print("\nâš ï¸ TensorFlowê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        
        # 8. ëª¨ë“  ëª¨ë¸ í‰ê°€
        best_model, best_model_name = evaluate_all_models(ml_results, dl_results, X_test_scaled, y_test, label_encoder)
        
        # 9. ê²°ê³¼ ì‹œê°í™”
        visualize_results(ml_results, dl_results, y_test, label_encoder)
        
        # 10. ëª¨ë¸ ì €ì¥
        import pickle
        model_save_path = pkl_path.replace('.pkl', '_best_model_with_dl.pkl')
        
        model_data = {
            'best_model': best_model,
            'best_model_name': best_model_name,
            'scaler': scaler,
            'label_encoder': label_encoder,
            'feature_columns': feature_columns,
            'ml_results': ml_results,
            'dl_results': dl_results if TENSORFLOW_AVAILABLE else None
        }
        
        with open(model_save_path, 'wb') as f:
            pickle.dump(model_data, f)
        
        print(f"\nğŸ’¾ ìµœê³  ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_save_path}")
        print(f"ëª¨ë¸: {best_model_name}")
        
        if best_model_name in ml_results:
            print(f"ì„±ëŠ¥: {ml_results[best_model_name]['accuracy']:.4f}")
        else:
            print(f"ì„±ëŠ¥: {dl_results[best_model_name]['accuracy']:.4f}")
        
        print("\nğŸ‰ ìŠ¤íƒ ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()